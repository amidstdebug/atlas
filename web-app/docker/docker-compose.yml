services:
  backend_api:
    platform: linux/amd64   # if needed for platform emulation
    build:
      context: ../backend
      dockerfile: Dockerfile
    volumes:
      - ../backend:/usr/src/app  # mount server code for hot reload
    ports:
      - "5002:5002"  # Main API port
    env_file:
      - ../backend/.env
    environment:
      PYTHONUNBUFFERED: 1
      WHISPER_SERVICE_URL: "http://whisper_service:8000"
      REDIS_URL: "redis://redis:6379/0"
    depends_on:
      whisper_service:
        condition: service_healthy
      summarization_service:
        condition: service_healthy
      redis:
        condition: service_started
    networks:
      - auto_transcript_network

  whisper_service:
    platform: linux/amd64
    build:
      context: ./services/whisper
      dockerfile: Dockerfile
    ports:
      - "8000:8000"  # Whisper service port
    environment:
      PYTHONUNBUFFERED: 1
      WHISPER_MODEL_SIZE: "large-v3-turbo"  # Options: tiny, base, small, medium, large
      CUDA_VISIBLE_DEVICES: 0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - auto_transcript_network

  summarization_service:
    image: ollama/ollama
    container_name: ollama
    runtime: nvidia  # To use GPUs
    environment:
      - NVIDIA_VISIBLE_DEVICES=all  # Ensures the container can see all GPUs
      - OLLAMA_KEEP_ALIVE=-1
    volumes:
      - ollama_data:/root/.ollama
      - ./services/ollama:/root/ollama_scripts  # Mount scripts directory to the container
    ports:
      - "11434:11434"  # Expose the Ollama service on port 11434
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    restart: "no"
    entrypoint: ["bash", "/root/ollama_scripts/run_ollama.sh"]
    healthcheck:
      test: ["CMD", "test", "-f", "/tmp/ollama_ready"]
      interval: 10s
      timeout: 5s
      retries: 10
    networks:
      - auto_transcript_network

  redis:
    image: redis:7
    restart: unless-stopped
    networks:
      - auto_transcript_network

  whisper_worker:
    build:
      context: ../backend
      dockerfile: Dockerfile
    command: python -m services.whisper.worker
    env_file:
      - ../backend/.env
    depends_on:
      - redis
      - whisper_service
    networks:
      - auto_transcript_network

  summary_worker:
    build:
      context: ../backend
      dockerfile: Dockerfile
    command: python -m services.summary.worker
    env_file:
      - ../backend/.env
    depends_on:
      - redis
      - summarization_service
    networks:
      - auto_transcript_network

  frontend:
    build:
      context: ../frontend
      dockerfile: Dockerfile
    ports:
      - "8080:80"  # Serve on port 80 in production mode
    depends_on:
      - backend_api
    networks:
      - auto_transcript_network

volumes:
  ollama_data:
    driver: local  # Persistent volume for Ollama

networks:
  auto_transcript_network:
    driver: bridge
