services:
  backend_api:
    platform: linux/amd64   # if needed for platform emulation
    build:
      context: ../backend
      dockerfile: Dockerfile
    volumes:
      - ../backend:/usr/src/app  # mount server code for hot reload
    ports:
      - "5002:5002"  # Main API port
    env_file:
      - ../backend/.env
    environment:
      PYTHONUNBUFFERED: 1
      WHISPER_SERVICE_URL: "http://whisper_service:8000"
      VLLM_SERVER_URL: "http://vllm_service:8000"
      REDIS_URL: "redis://redis:6379/0"
    depends_on:
      whisper_service:
        condition: service_healthy
      vllm_service:
        condition: service_healthy
      redis:
        condition: service_started
    networks:
      - auto_transcript_network

  whisper_service:
    platform: linux/amd64
    build:
      context: ./services/whisper
      dockerfile: Dockerfile
    ports:
      - "8000:8000"  # Whisper service port
    environment:
      PYTHONUNBUFFERED: 1
      WHISPER_MODEL_ID: "jlvdoorn/whisper-large-v3-atco2-asr"
      CUDA_VISIBLE_DEVICES: 0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "healthcheck.py"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 120s
    volumes:
      - ../backend:/usr/src/app  # mount server code for hot reload
      - ./models:/root/.cache/huggingface  # persist HuggingFace cache
      - ./models:/models  # persist model files
    networks:
      - auto_transcript_network

  vllm_service:
    image: vllm/vllm-openai:latest
    container_name: vllm
    runtime: nvidia  # To use GPUs
    env_file:
      - ../backend/.env
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    command: >
      --model ${VLLM_MODEL:-Qwen/Qwen3-0.6B}
      --gpu-memory-utilization ${VLLM_GPU_MEMORY_UTILIZATION:-0.7}
      --max-model-len ${VLLM_MAX_MODEL_LEN:-2048}
      --tensor-parallel-size ${VLLM_TENSOR_PARALLEL_SIZE:-1}
      --host 0.0.0.0
      --port 8000
    ports:
      - "8001:8000"  # vLLM service exposed on host port 8001, container port 8000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 500s
    volumes:
      - ./models:/root/.cache/huggingface  # persist HuggingFace cache
      - ./models:/models  # persist model files
    networks:
      - auto_transcript_network

  redis:
    image: redis:7
    restart: unless-stopped
    networks:
      - auto_transcript_network

  whisper_worker:
    build:
      context: ../backend
      dockerfile: Dockerfile
    command: python -m services.whisper.worker
    env_file:
      - ../backend/.env
    depends_on:
      - redis
      - whisper_service
    networks:
      - auto_transcript_network

  summary_worker:
    build:
      context: ../backend
      dockerfile: Dockerfile
    command: python -m services.summary.worker
    env_file:
      - ../backend/.env
    depends_on:
      - redis
      - vllm_service
    networks:
      - auto_transcript_network

  frontend:
    build:
      context: ../frontend
      dockerfile: Dockerfile
    ports:
      - "8080:80"  # Serve on port 80 in production mode
    depends_on:
      - backend_api
    networks:
      - auto_transcript_network

networks:
  auto_transcript_network:
    driver: bridge

volumes:
  models: