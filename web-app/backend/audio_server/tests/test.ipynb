{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0201 13:52:25.677588 139686079022208 zarr.py:57] `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n"
     ]
    }
   ],
   "source": [
    "# inference/asr_infer.py\n",
    "import os\n",
    "import json\n",
    "import wget\n",
    "import tempfile\n",
    "from omegaconf import OmegaConf\n",
    "from nemo.collections.asr.parts.utils.decoder_timestamps_utils import ASRDecoderTimeStamps\n",
    "from nemo.collections.asr.parts.utils.diarization_utils import OfflineDiarWithASR\n",
    "\n",
    "def load_config(config_dir, domain_type=\"meeting\"):\n",
    "\t\"\"\"\n",
    "\tLoad (or download if not present) the diarization inference configuration file.\n",
    "\tThe configuration file is expected to reside in `config_dir`. If not present, it is downloaded.\n",
    "\t\"\"\"\n",
    "\tconfig_filename = f\"diar_infer_{domain_type}.yaml\"\n",
    "\tconfig_path = os.path.join(config_dir, config_filename)\n",
    "\tconfig_url = (\n",
    "\t\tf\"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/{config_filename}\"\n",
    "\t)\n",
    "\t\n",
    "\tif not os.path.exists(config_path):\n",
    "\t\tprint(f\"Downloading config from {config_url}...\")\n",
    "\t\twget.download(config_url, out=config_dir)\n",
    "\t\n",
    "\tcfg = OmegaConf.load(config_path)\n",
    "\treturn cfg\n",
    "\n",
    "def create_manifest(audio_file_path, out_dir):\n",
    "\t\"\"\"\n",
    "\tCreate a manifest file as required by the diarization pipeline.\n",
    "\t\n",
    "\tThis function writes the manifest file to the provided `out_dir` (which, in our updated\n",
    "\tinference code, will be a temporary directory). The manifest contains the necessary fields:\n",
    "\t  - audio_filepath\n",
    "\t  - offset\n",
    "\t  - duration\n",
    "\t  - label (set to 'infer')\n",
    "\t  - text (placeholder)\n",
    "\t  - num_speakers (None)\n",
    "\t  - rttm_filepath (None)\n",
    "\t  - uem_filepath (None)\n",
    "\t\"\"\"\n",
    "\tmanifest_data = {\n",
    "\t\t'audio_filepath': audio_file_path,\n",
    "\t\t'offset': 0,\n",
    "\t\t'duration': None,\n",
    "\t\t'label': 'infer',\n",
    "\t\t'text': '-',\n",
    "\t\t'num_speakers': None,\n",
    "\t\t'rttm_filepath': None,\n",
    "\t\t'uem_filepath': None\n",
    "\t}\n",
    "\tmanifest_path = os.path.join(out_dir, 'input_manifest.json')\n",
    "\twith open(manifest_path, 'w') as fp:\n",
    "\t\tjson.dump(manifest_data, fp)\n",
    "\t\tfp.write('\\n')\n",
    "\treturn manifest_path\n",
    "\n",
    "def run_inference(audio_file_path: str, config_dir: str):\n",
    "\t\"\"\"\n",
    "\tRun ASR and Speaker Diarization inference on the provided audio file.\n",
    "\t\n",
    "\tThis function uses a temporary directory for all intermediate files (including the manifest and\n",
    "\tthe prediction outputs), ensuring that no persistent files are left behind across inference\n",
    "\tinvocations.\n",
    "\t\n",
    "\tParameters:\n",
    "\t  audio_file_path (str): The path to the input audio file.\n",
    "\t  config_dir (str): The directory where configuration files (e.g., diar_infer_meeting.yaml) reside.\n",
    "\t\n",
    "\tReturns:\n",
    "\t  dict: A dictionary containing:\n",
    "\t\t\t- \"transcript\": The final transcript text with speaker labels.\n",
    "\t\t\t- \"rttm\": The RTTM content describing speaker segments.\n",
    "\t\t\t- \"transcript_info\": Additional detailed transcript information from the pipeline.\n",
    "\t\"\"\"\n",
    "\t# Use a temporary directory for all intermediate and output files.\n",
    "\twith tempfile.TemporaryDirectory() as temp_out_dir:\n",
    "\t\t# Create a subdirectory to mimic the \"pred_rttms\" structure used by the pipeline.\n",
    "\t\tpred_rttms_dir = os.path.join(temp_out_dir, 'pred_rttms')\n",
    "\t\tos.makedirs(pred_rttms_dir, exist_ok=True)\n",
    "\t\t\n",
    "\t\t# Create the manifest file in the temporary output directory.\n",
    "\t\tmanifest_path = create_manifest(audio_file_path, temp_out_dir)\n",
    "\t\t\n",
    "\t\t# Load the diarization configuration from the provided config directory.\n",
    "\t\tcfg = load_config(config_dir, domain_type=\"meeting\")\n",
    "\t\t\n",
    "\t\t# Update the configuration with our temporary manifest and output directory.\n",
    "\t\tcfg.diarizer.manifest_filepath = manifest_path\n",
    "\t\tcfg.diarizer.out_dir = temp_out_dir\n",
    "\t\t\n",
    "\t\t# Set additional configuration parameters.\n",
    "\t\tcfg.diarizer.speaker_embeddings.model_path = 'titanet_large'\n",
    "\t\tcfg.diarizer.clustering.parameters.oracle_num_speakers = False\n",
    "\t\tcfg.diarizer.vad.model_path = 'vad_multilingual_marblenet'\n",
    "\t\tcfg.diarizer.asr.model_path = 'stt_en_conformer_ctc_large'\n",
    "\t\tcfg.diarizer.oracle_vad = False\n",
    "\t\tcfg.diarizer.asr.parameters.asr_based_vad = False\n",
    "\n",
    "\t\t# Run ASR to obtain word hypotheses and word-level timestamps.\n",
    "\t\tasr_decoder = ASRDecoderTimeStamps(cfg.diarizer)\n",
    "\n",
    "\t\t# # asr_model = asr_decoder.set_asr_model()\n",
    "\t\t# import nemo.collections.asr as nemo_asr\n",
    "\t\t# asr_model = nemo_asr.models.ASRModel.from_pretrained(\"stt_en_conformer_ctc_large\")\n",
    "\n",
    "\t\tasr_model = asr_decoder.set_asr_model()\n",
    "\t\tword_hyp, word_ts_hyp = asr_decoder.run_ASR(asr_model)\n",
    "\t\tprint(\"OUT:\", word_hyp, word_ts_hyp)\n",
    "\t\t\n",
    "\t\t# Run diarization using the ASR output timestamps.\n",
    "\t\tasr_diar = OfflineDiarWithASR(cfg.diarizer)\n",
    "\t\tasr_diar.word_ts_anchor_offset = asr_decoder.word_ts_anchor_offset\n",
    "\t\tdiar_hyp, _ = asr_diar.run_diarization(cfg, word_ts_hyp)\n",
    "\t\t\n",
    "\t\t# Merge diarization and ASR results to get the final transcript with speaker labels.\n",
    "\t\ttranscript_info = asr_diar.get_transcript_with_speaker_labels(diar_hyp, word_hyp, word_ts_hyp)\n",
    "\t\t\n",
    "\t\t# Determine the base name (without the .wav extension) for the output files.\n",
    "\t\tbase_name = os.path.basename(audio_file_path).replace('.wav', '')\n",
    "\t\ttranscript_file = os.path.join(pred_rttms_dir, f\"{base_name}.txt\")\n",
    "\t\trttm_file = os.path.join(pred_rttms_dir, f\"{base_name}.rttm\")\n",
    "\t\t\n",
    "\t\t# Read the transcript (if it was written by the pipeline).\n",
    "\t\ttranscript = \"\"\n",
    "\t\tif os.path.exists(transcript_file):\n",
    "\t\t\twith open(transcript_file, \"r\") as f:\n",
    "\t\t\t\ttranscript = f.read()\n",
    "\t\t\n",
    "\t\t# Read the RTTM file (if it was generated).\n",
    "\t\trttm = \"\"\n",
    "\t\tif os.path.exists(rttm_file):\n",
    "\t\t\twith open(rttm_file, \"r\") as f:\n",
    "\t\t\t\trttm = f.read()\n",
    "\t\t\n",
    "\t\t# Construct and return the result dictionary.\n",
    "\t\tresult = {\n",
    "\t\t\t\"transcript\": transcript,\n",
    "\t\t\t\"rttm\": rttm,\n",
    "\t\t\t\"transcript_info\": transcript_info\n",
    "\t\t}\n",
    "\t\t\n",
    "\t\t# The temporary directory (and all its files) is automatically cleaned up here.\n",
    "\t\treturn result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-01 13:52:26 speaker_utils:93] Number of files to diarize: 1\n",
      "[NeMo I 2025-02-01 13:52:26 cloud:58] Found existing object /root/.cache/torch/NeMo/NeMo_2.0.0rc1/stt_en_conformer_ctc_large/afb212c5bcf904e326b5e5751e7c7465/stt_en_conformer_ctc_large.nemo.\n",
      "[NeMo I 2025-02-01 13:52:26 cloud:64] Re-using file from: /root/.cache/torch/NeMo/NeMo_2.0.0rc1/stt_en_conformer_ctc_large/afb212c5bcf904e326b5e5751e7c7465/stt_en_conformer_ctc_large.nemo\n",
      "[NeMo I 2025-02-01 13:52:26 common:826] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2025-02-01 13:52:27 mixins:173] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-02-01 13:52:27 modelPT:176] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath:\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket1/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket2/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket3/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket4/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket5/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket6/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket7/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket8/tarred_audio_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 1\n",
      "    shuffle: true\n",
      "    num_workers: 4\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20.0\n",
      "    min_duration: 0.1\n",
      "    is_tarred: true\n",
      "    tarred_audio_filepaths:\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket1/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket2/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket3/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket4/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket5/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket6/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket7/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket8/audio__OP_0..8191_CL_.tar\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: synced_randomized\n",
      "    bucketing_batch_size:\n",
      "    - 34\n",
      "    - 30\n",
      "    - 26\n",
      "    - 22\n",
      "    - 18\n",
      "    - 16\n",
      "    - 12\n",
      "    - 8\n",
      "    \n",
      "[NeMo W 2025-02-01 13:52:27 modelPT:183] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath:\n",
      "    - /manifests/librispeech/librivox-dev-other.json\n",
      "    - /manifests/librispeech/librivox-dev-clean.json\n",
      "    - /manifests/librispeech/librivox-test-other.json\n",
      "    - /manifests/librispeech/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n",
      "[NeMo W 2025-02-01 13:52:27 modelPT:189] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath:\n",
      "    - /manifests/librispeech/librivox-dev-other.json\n",
      "    - /manifests/librispeech/librivox-dev-clean.json\n",
      "    - /manifests/librispeech/librivox-test-other.json\n",
      "    - /manifests/librispeech/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-01 13:52:27 features:305] PADDING: 0\n",
      "[NeMo I 2025-02-01 13:52:28 save_restore_connector:272] Model EncDecCTCModelBPE was successfully restored from /root/.cache/torch/NeMo/NeMo_2.0.0rc1/stt_en_conformer_ctc_large/afb212c5bcf904e326b5e5751e7c7465/stt_en_conformer_ctc_large.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-02-01 13:52:28 decoder_timestamps_utils:71] `ctc_decode` was set to True. Note that this is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-01 13:52:28 features:305] PADDING: 0\n",
      "[NeMo I 2025-02-01 13:52:28 features:305] PADDING: 0\n",
      "[NeMo I 2025-02-01 13:52:28 decoder_timestamps_utils:664] Running ASR model stt_en_conformer_ctc_large\n",
      "[NeMo I 2025-02-01 13:52:28 decoder_timestamps_utils:668] [1/1] FrameBatchASR: ../an4_diarize_test.wav\n",
      "{'an4_diarize_test': ['eleven', 'twenty', 'seven', 'fifty', 'seven', 'october', 'twenty', 'four', 'nineteen', 'seventy']} {'an4_diarize_test': [[0.36, 0.72], [0.92, 1.28], [1.4, 1.64], [1.96, 2.28], [2.36, 2.6], [3.08, 3.52], [3.6, 3.84], [3.88, 4.04], [4.4, 4.72], [4.84, 5.16]]}\n",
      "[NeMo I 2025-02-01 13:52:30 speaker_utils:93] Number of files to diarize: 1\n",
      "[NeMo I 2025-02-01 13:52:30 clustering_diarizer:127] Loading pretrained vad_multilingual_marblenet model from NGC\n",
      "[NeMo I 2025-02-01 13:52:30 cloud:58] Found existing object /root/.cache/torch/NeMo/NeMo_2.0.0rc1/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo.\n",
      "[NeMo I 2025-02-01 13:52:30 cloud:64] Re-using file from: /root/.cache/torch/NeMo/NeMo_2.0.0rc1/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo\n",
      "[NeMo I 2025-02-01 13:52:30 common:826] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-02-01 13:52:30 modelPT:176] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /manifests/ami_train_0.63.json,/manifests/freesound_background_train.json,/manifests/freesound_laughter_train.json,/manifests/fisher_2004_background.json,/manifests/fisher_2004_speech_sampled.json,/manifests/google_train_manifest.json,/manifests/icsi_all_0.63.json,/manifests/musan_freesound_train.json,/manifests/musan_music_train.json,/manifests/musan_soundbible_train.json,/manifests/mandarin_train_sample.json,/manifests/german_train_sample.json,/manifests/spanish_train_sample.json,/manifests/french_train_sample.json,/manifests/russian_train_sample.json\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 256\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "    augmentor:\n",
      "      shift:\n",
      "        prob: 0.5\n",
      "        min_shift_ms: -10.0\n",
      "        max_shift_ms: 10.0\n",
      "      white_noise:\n",
      "        prob: 0.5\n",
      "        min_level: -90\n",
      "        max_level: -46\n",
      "        norm: true\n",
      "      noise:\n",
      "        prob: 0.5\n",
      "        manifest_path: /manifests/noise_0_1_musan_fs.json\n",
      "        min_snr_db: 0\n",
      "        max_snr_db: 30\n",
      "        max_gain_db: 300.0\n",
      "        norm: true\n",
      "      gain:\n",
      "        prob: 0.5\n",
      "        min_gain_dbfs: -10.0\n",
      "        max_gain_dbfs: 10.0\n",
      "        norm: true\n",
      "    num_workers: 16\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2025-02-01 13:52:30 modelPT:183] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /manifests/ami_dev_0.63.json,/manifests/freesound_background_dev.json,/manifests/freesound_laughter_dev.json,/manifests/ch120_moved_0.63.json,/manifests/fisher_2005_500_speech_sampled.json,/manifests/google_dev_manifest.json,/manifests/musan_music_dev.json,/manifests/mandarin_dev.json,/manifests/german_dev.json,/manifests/spanish_dev.json,/manifests/french_dev.json,/manifests/russian_dev.json\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 256\n",
      "    shuffle: false\n",
      "    val_loss_idx: 0\n",
      "    num_workers: 16\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2025-02-01 13:52:30 modelPT:189] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "    test_loss_idx: 0\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-01 13:52:30 features:305] PADDING: 16\n",
      "[NeMo I 2025-02-01 13:52:30 save_restore_connector:272] Model EncDecClassificationModel was successfully restored from /root/.cache/torch/NeMo/NeMo_2.0.0rc1/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo.\n",
      "[NeMo I 2025-02-01 13:52:30 clustering_diarizer:160] Loading pretrained titanet_large model from NGC\n",
      "[NeMo I 2025-02-01 13:52:30 cloud:58] Found existing object /root/.cache/torch/NeMo/NeMo_2.0.0rc1/titanet-l/11ba0924fdf87c049e339adbf6899d48/titanet-l.nemo.\n",
      "[NeMo I 2025-02-01 13:52:30 cloud:64] Re-using file from: /root/.cache/torch/NeMo/NeMo_2.0.0rc1/titanet-l/11ba0924fdf87c049e339adbf6899d48/titanet-l.nemo\n",
      "[NeMo I 2025-02-01 13:52:30 common:826] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-02-01 13:52:30 modelPT:176] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /manifests/combined_fisher_swbd_voxceleb12_librispeech/train.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "    augmentor:\n",
      "      noise:\n",
      "        manifest_path: /manifests/noise/rir_noise_manifest.json\n",
      "        prob: 0.5\n",
      "        min_snr_db: 0\n",
      "        max_snr_db: 15\n",
      "      speed:\n",
      "        prob: 0.5\n",
      "        sr: 16000\n",
      "        resample_type: kaiser_fast\n",
      "        min_speed_rate: 0.95\n",
      "        max_speed_rate: 1.05\n",
      "    num_workers: 15\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2025-02-01 13:52:30 modelPT:183] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /manifests/combined_fisher_swbd_voxceleb12_librispeech/dev.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "    num_workers: 15\n",
      "    pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-01 13:52:30 features:305] PADDING: 16\n",
      "[NeMo I 2025-02-01 13:52:30 save_restore_connector:272] Model EncDecSpeakerLabelModel was successfully restored from /root/.cache/torch/NeMo/NeMo_2.0.0rc1/titanet-l/11ba0924fdf87c049e339adbf6899d48/titanet-l.nemo.\n",
      "[NeMo I 2025-02-01 13:52:30 speaker_utils:93] Number of files to diarize: 1\n",
      "[NeMo I 2025-02-01 13:52:30 clustering_diarizer:313] Split long audio file to avoid CUDA memory issue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "splitting manifest: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-01 13:52:31 classification_models:293] Perform streaming frame-level VAD\n",
      "[NeMo I 2025-02-01 13:52:31 collections:740] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2025-02-01 13:52:31 collections:741] Dataset successfully loaded with 1 items and total duration provided from manifest is  0.00 hours.\n",
      "[NeMo I 2025-02-01 13:52:31 collections:746] # 1 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "vad: 100%|██████████| 1/1 [00:00<00:00,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-01 13:52:31 clustering_diarizer:266] Converting frame level prediction to speech/no-speech segment in start and end times format.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "creating speech segments: 100%|██████████| 1/1 [00:00<00:00, 23.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-01 13:52:31 clustering_diarizer:291] Subsegmentation for embedding extraction: scale0, /tmp/tmpgop083t7/speaker_outputs/subsegments_scale0.json\n",
      "[NeMo I 2025-02-01 13:52:31 clustering_diarizer:347] Extracting embeddings for Diarization\n",
      "[NeMo I 2025-02-01 13:52:31 collections:740] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2025-02-01 13:52:31 collections:741] Dataset successfully loaded with 3 items and total duration provided from manifest is  0.00 hours.\n",
      "[NeMo I 2025-02-01 13:52:31 collections:746] # 3 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/6] extract embeddings: 100%|██████████| 1/1 [00:00<00:00,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-01 13:52:32 clustering_diarizer:393] Saved embedding files to /tmp/tmpgop083t7/speaker_outputs/embeddings\n",
      "[NeMo I 2025-02-01 13:52:32 clustering_diarizer:291] Subsegmentation for embedding extraction: scale1, /tmp/tmpgop083t7/speaker_outputs/subsegments_scale1.json\n",
      "[NeMo I 2025-02-01 13:52:32 clustering_diarizer:347] Extracting embeddings for Diarization\n",
      "[NeMo I 2025-02-01 13:52:32 collections:740] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2025-02-01 13:52:32 collections:741] Dataset successfully loaded with 4 items and total duration provided from manifest is  0.00 hours.\n",
      "[NeMo I 2025-02-01 13:52:32 collections:746] # 4 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/6] extract embeddings: 100%|██████████| 1/1 [00:00<00:00,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-01 13:52:32 clustering_diarizer:393] Saved embedding files to /tmp/tmpgop083t7/speaker_outputs/embeddings\n",
      "[NeMo I 2025-02-01 13:52:32 clustering_diarizer:291] Subsegmentation for embedding extraction: scale2, /tmp/tmpgop083t7/speaker_outputs/subsegments_scale2.json\n",
      "[NeMo I 2025-02-01 13:52:32 clustering_diarizer:347] Extracting embeddings for Diarization\n",
      "[NeMo I 2025-02-01 13:52:32 collections:740] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2025-02-01 13:52:32 collections:741] Dataset successfully loaded with 5 items and total duration provided from manifest is  0.00 hours.\n",
      "[NeMo I 2025-02-01 13:52:32 collections:746] # 5 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/6] extract embeddings: 100%|██████████| 1/1 [00:00<00:00,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-01 13:52:32 clustering_diarizer:393] Saved embedding files to /tmp/tmpgop083t7/speaker_outputs/embeddings\n",
      "[NeMo I 2025-02-01 13:52:32 clustering_diarizer:291] Subsegmentation for embedding extraction: scale3, /tmp/tmpgop083t7/speaker_outputs/subsegments_scale3.json\n",
      "[NeMo I 2025-02-01 13:52:32 clustering_diarizer:347] Extracting embeddings for Diarization\n",
      "[NeMo I 2025-02-01 13:52:32 collections:740] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2025-02-01 13:52:32 collections:741] Dataset successfully loaded with 6 items and total duration provided from manifest is  0.00 hours.\n",
      "[NeMo I 2025-02-01 13:52:32 collections:746] # 6 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/6] extract embeddings: 100%|██████████| 1/1 [00:00<00:00,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-01 13:52:32 clustering_diarizer:393] Saved embedding files to /tmp/tmpgop083t7/speaker_outputs/embeddings\n",
      "[NeMo I 2025-02-01 13:52:32 clustering_diarizer:291] Subsegmentation for embedding extraction: scale4, /tmp/tmpgop083t7/speaker_outputs/subsegments_scale4.json\n",
      "[NeMo I 2025-02-01 13:52:32 clustering_diarizer:347] Extracting embeddings for Diarization\n",
      "[NeMo I 2025-02-01 13:52:32 collections:740] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2025-02-01 13:52:32 collections:741] Dataset successfully loaded with 10 items and total duration provided from manifest is  0.00 hours.\n",
      "[NeMo I 2025-02-01 13:52:32 collections:746] # 10 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/6] extract embeddings: 100%|██████████| 1/1 [00:00<00:00,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-01 13:52:32 clustering_diarizer:393] Saved embedding files to /tmp/tmpgop083t7/speaker_outputs/embeddings\n",
      "[NeMo I 2025-02-01 13:52:32 clustering_diarizer:291] Subsegmentation for embedding extraction: scale5, /tmp/tmpgop083t7/speaker_outputs/subsegments_scale5.json\n",
      "[NeMo I 2025-02-01 13:52:32 clustering_diarizer:347] Extracting embeddings for Diarization\n",
      "[NeMo I 2025-02-01 13:52:32 collections:740] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2025-02-01 13:52:32 collections:741] Dataset successfully loaded with 20 items and total duration provided from manifest is  0.00 hours.\n",
      "[NeMo I 2025-02-01 13:52:32 collections:746] # 20 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[6/6] extract embeddings: 100%|██████████| 1/1 [00:00<00:00,  6.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-01 13:52:32 clustering_diarizer:393] Saved embedding files to /tmp/tmpgop083t7/speaker_outputs/embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "clustering: 100%|██████████| 1/1 [00:00<00:00,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-01 13:52:33 clustering_diarizer:461] Outputs are saved in /tmp/tmpgop083t7 directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[NeMo W 2025-02-01 13:52:33 der:185] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-01 13:52:33 diarization_utils:876] Creating results for Session: an4_diarize_test n_spk: 2 \n",
      "[NeMo I 2025-02-01 13:52:33 diarization_utils:749] Diarization with ASR output files are saved in: /tmp/tmpgop083t7/pred_rttms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'transcript': '[00:00.07 - 00:02.60] speaker_0: eleven twenty seven fifty seven\\n[00:03.08 - 00:05.16] speaker_1: october twenty four nineteen seventy\\n',\n",
       " 'rttm': 'SPEAKER an4_diarize_test 1   0.070   2.625 <NA> <NA> speaker_0 <NA> <NA>\\nSPEAKER an4_diarize_test 1   2.695   2.505 <NA> <NA> speaker_1 <NA> <NA>\\n',\n",
       " 'transcript_info': {'an4_diarize_test': OrderedDict([('status', 'success'),\n",
       "               ('session_id', 'an4_diarize_test'),\n",
       "               ('transcription',\n",
       "                'eleven twenty seven fifty seven october twenty four nineteen seventy'),\n",
       "               ('speaker_count', 2),\n",
       "               ('words',\n",
       "                [{'word': 'eleven',\n",
       "                  'start_time': 0.36,\n",
       "                  'end_time': 0.72,\n",
       "                  'speaker': 'speaker_0'},\n",
       "                 {'word': 'twenty',\n",
       "                  'start_time': 0.92,\n",
       "                  'end_time': 1.28,\n",
       "                  'speaker': 'speaker_0'},\n",
       "                 {'word': 'seven',\n",
       "                  'start_time': 1.4,\n",
       "                  'end_time': 1.64,\n",
       "                  'speaker': 'speaker_0'},\n",
       "                 {'word': 'fifty',\n",
       "                  'start_time': 1.96,\n",
       "                  'end_time': 2.28,\n",
       "                  'speaker': 'speaker_0'},\n",
       "                 {'word': 'seven',\n",
       "                  'start_time': 2.36,\n",
       "                  'end_time': 2.6,\n",
       "                  'speaker': 'speaker_0'},\n",
       "                 {'word': 'october',\n",
       "                  'start_time': 3.08,\n",
       "                  'end_time': 3.52,\n",
       "                  'speaker': 'speaker_1'},\n",
       "                 {'word': 'twenty',\n",
       "                  'start_time': 3.6,\n",
       "                  'end_time': 3.84,\n",
       "                  'speaker': 'speaker_1'},\n",
       "                 {'word': 'four',\n",
       "                  'start_time': 3.88,\n",
       "                  'end_time': 4.04,\n",
       "                  'speaker': 'speaker_1'},\n",
       "                 {'word': 'nineteen',\n",
       "                  'start_time': 4.4,\n",
       "                  'end_time': 4.72,\n",
       "                  'speaker': 'speaker_1'},\n",
       "                 {'word': 'seventy',\n",
       "                  'start_time': 4.84,\n",
       "                  'end_time': 5.16,\n",
       "                  'speaker': 'speaker_1'}]),\n",
       "               ('sentences',\n",
       "                [{'speaker': 'speaker_0',\n",
       "                  'start_time': '0.07',\n",
       "                  'end_time': 2.6,\n",
       "                  'text': 'eleven twenty seven fifty seven'},\n",
       "                 {'speaker': 'speaker_1',\n",
       "                  'start_time': 3.08,\n",
       "                  'end_time': 5.16,\n",
       "                  'text': 'october twenty four nineteen seventy'}])])}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "\n",
    "run_inference('../an4_diarize_test.wav', config_dir = DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-01 16:03:54 mixins:173] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-02-01 16:03:54 modelPT:176] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /disk1/NVIDIA/datasets/LibriSpeech_NeMo/librivox-train-all.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 16.7\n",
      "    min_duration: 0.1\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: fully_randomized\n",
      "    bucketing_batch_size: null\n",
      "    \n",
      "[NeMo W 2025-02-01 16:03:54 modelPT:183] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /disk1/NVIDIA/datasets/LibriSpeech_NeMo/librivox-dev-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    use_start_end_token: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2025-02-01 16:03:54 modelPT:189] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    use_start_end_token: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-01 16:03:54 features:305] PADDING: 0\n",
      "[NeMo I 2025-02-01 16:04:04 save_restore_connector:272] Model EncDecCTCModelBPE was successfully restored from /root/.cache/huggingface/hub/models--nvidia--parakeet-ctc-1.1b/snapshots/085a3de63c7598065b072cd8f2182e6a5fa593eb/parakeet-ctc-1.1b.nemo.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncDecCTCModelBPE(\n",
       "  (preprocessor): AudioToMelSpectrogramPreprocessor(\n",
       "    (featurizer): FilterbankFeatures()\n",
       "  )\n",
       "  (encoder): ConformerEncoder(\n",
       "    (pre_encode): ConvSubsampling(\n",
       "      (out): Linear(in_features=2560, out_features=1024, bias=True)\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "        (6): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pos_enc): RelPositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-41): 42 x ConformerLayer(\n",
       "        (norm_feed_forward1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward1): ConformerFeedForward(\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (activation): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (norm_conv): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): ConformerConvolution(\n",
       "          (pointwise_conv1): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,))\n",
       "          (depthwise_conv): CausalConv1D(1024, 1024, kernel_size=(9,), stride=(1,), groups=1024)\n",
       "          (batch_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): Swish()\n",
       "          (pointwise_conv2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (norm_self_att): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): RelPositionMultiHeadAttention(\n",
       "          (linear_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (linear_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (linear_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (linear_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (norm_feed_forward2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward2): ConformerFeedForward(\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (activation): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm_out): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ConvASRDecoder(\n",
       "    (decoder_layers): Sequential(\n",
       "      (0): Conv1d(1024, 1025, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (loss): CTCLoss()\n",
       "  (spec_augmentation): SpectrogramAugmentation(\n",
       "    (spec_augment): SpecAugment()\n",
       "  )\n",
       "  (wer): WER()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "asr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(model_name=\"nvidia/parakeet-ctc-1.1b\")\n",
    "asr_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing:   0%|          | 0/1 [00:00<?, ?it/s][NeMo W 2025-02-01 16:04:08 ctc_greedy_decoding:168] CTC decoding strategy 'greedy' is slower than 'greedy_batch', which implements the same exact interface. Consider changing your strategy to 'greedy_batch' for a free performance improvement.\n",
      "Transcribing: 100%|██████████| 1/1 [00:02<00:00,  2.25s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"he frequently askks are you okay during sex show us show us like what does that look like are you okay i like when guys are confident to be able to compliment other people hey check out those balloons those calciumons don't put this in the trailer whoever add it this i'll tell you it com in the trailer\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_model.transcribe('../mono_output.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0208 12:48:11.759294 139677262001280 zarr.py:57] `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n"
     ]
    }
   ],
   "source": [
    "from nemo.collections.asr.models import EncDecMultiTaskModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-08 12:48:20 mixins:197] _setup_tokenizer: detected an aggregate tokenizer\n",
      "[NeMo I 2025-02-08 12:48:20 mixins:336] Tokenizer SentencePieceTokenizer initialized with 32 tokens\n",
      "[NeMo I 2025-02-08 12:48:20 mixins:336] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "[NeMo I 2025-02-08 12:48:20 mixins:336] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "[NeMo I 2025-02-08 12:48:20 mixins:336] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "[NeMo I 2025-02-08 12:48:20 mixins:336] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "[NeMo I 2025-02-08 12:48:20 aggregate_tokenizer:73] Aggregate vocab size: 4128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-02-08 12:48:20 modelPT:176] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    tarred_audio_filepaths: null\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    shuffle: true\n",
      "    batch_size: null\n",
      "    num_workers: 8\n",
      "    use_lhotse: true\n",
      "    max_duration: 40\n",
      "    pin_memory: true\n",
      "    use_bucketing: false\n",
      "    bucket_duration_bins: null\n",
      "    num_buckets: 1\n",
      "    text_field: answer\n",
      "    lang_field: target_lang\n",
      "    batch_duration: 360\n",
      "    quadratic_duration: 15\n",
      "    bucket_buffer_size: 20000\n",
      "    shuffle_buffer_size: 10000\n",
      "    \n",
      "[NeMo W 2025-02-08 12:48:20 modelPT:183] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 8\n",
      "    shuffle: false\n",
      "    num_workers: 0\n",
      "    pin_memory: true\n",
      "    tarred_audio_filepaths: null\n",
      "    use_lhotse: true\n",
      "    text_field: answer\n",
      "    lang_field: target_lang\n",
      "    use_bucketing: false\n",
      "    \n",
      "[NeMo W 2025-02-08 12:48:20 modelPT:189] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 0\n",
      "    pin_memory: true\n",
      "    tarred_audio_filepaths: null\n",
      "    use_lhotse: true\n",
      "    text_field: answer\n",
      "    lang_field: target_lang\n",
      "    use_bucketing: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-08 12:48:20 features:305] PADDING: 0\n",
      "[NeMo I 2025-02-08 12:48:30 save_restore_connector:272] Model EncDecMultiTaskModel was successfully restored from /root/.cache/huggingface/hub/models--nvidia--canary-1b/snapshots/dd32c0c709e2bfc79f583e16b9df4b3a160f7e86/canary-1b.nemo.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "canary_model = EncDecMultiTaskModel.from_pretrained('nvidia/canary-1b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-08 12:48:33 aed_multitask_models:260] Changed decoding strategy to \n",
      "    strategy: beam\n",
      "    compute_hypothesis_token_set: false\n",
      "    preserve_alignments: null\n",
      "    compute_langs: false\n",
      "    beam:\n",
      "      beam_size: 1\n",
      "      search_type: default\n",
      "      len_pen: 1.0\n",
      "      max_generation_delta: 20\n",
      "      return_best_hypothesis: true\n",
      "      preserve_alignments: false\n",
      "    temperature: 1.0\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncDecMultiTaskModel(\n",
       "  (preprocessor): AudioToMelSpectrogramPreprocessor(\n",
       "    (featurizer): FilterbankFeatures()\n",
       "  )\n",
       "  (encoder): ConformerEncoder(\n",
       "    (pre_encode): ConvSubsampling(\n",
       "      (out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "        (6): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pos_enc): RelPositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x ConformerLayer(\n",
       "        (norm_feed_forward1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward1): ConformerFeedForward(\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (activation): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (norm_conv): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): ConformerConvolution(\n",
       "          (pointwise_conv1): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,))\n",
       "          (depthwise_conv): CausalConv1D(1024, 1024, kernel_size=(9,), stride=(1,), groups=1024)\n",
       "          (batch_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): Swish()\n",
       "          (pointwise_conv2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (norm_self_att): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): RelPositionMultiHeadAttention(\n",
       "          (linear_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (linear_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (linear_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (linear_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (norm_feed_forward2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward2): ConformerFeedForward(\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (activation): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm_out): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder_decoder_proj): Identity()\n",
       "  (transf_decoder): TransformerDecoderNM(\n",
       "    (_embedding): TransformerEmbedding(\n",
       "      (token_embedding): Embedding(4128, 1024, padding_idx=0)\n",
       "      (position_embedding): FixedPositionalEncoding()\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (_decoder): TransformerDecoder(\n",
       "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x TransformerDecoderBlock(\n",
       "          (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (first_sub_layer): MultiHeadAttention(\n",
       "            (query_net): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key_net): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value_net): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (second_sub_layer): MultiHeadAttention(\n",
       "            (query_net): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key_net): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value_net): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm_3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (third_sub_layer): PositionWiseFF(\n",
       "            (dense_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dense_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (layer_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (log_softmax): TokenClassifier(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mlp): MultiLayerPerceptron(\n",
       "      (layer0): Linear(in_features=1024, out_features=4128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss): SmoothedCrossEntropyLoss()\n",
       "  (spec_augmentation): SpectrogramAugmentation(\n",
       "    (spec_augment): SpecAugment()\n",
       "  )\n",
       "  (val_loss): GlobalAverageLossMetric()\n",
       "  (wer): WER()\n",
       "  (bleu): BLEU()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update dcode params\n",
    "decode_cfg = canary_model.cfg.decoding\n",
    "decode_cfg.beam.beam_size = 1\n",
    "canary_model.change_decoding_strategy(decode_cfg)\n",
    "canary_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing: 1it [00:05,  5.54s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"He frequently asks, Are you okay during sex? Show us, show us. What does that look like? Are you okay? I like when guys are confident to be able to compliment other people. Hey, check out those balloons. Those calcium balloons. Don't put this in the trailer. Whoever added this, I'll tell you. You confirm in the trailer.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canary_model.transcribe('../mono_output.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing: 1it [00:02,  2.99s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"He frequently asks, Are you okay during sex? Show us, show us. What does that look like? Are you okay? I like when guys are confident to be able to compliment other people. Hey, check out those balloons. Those calcium balloons. Don't put this in the trailer. Whoever added this, I'll tell you. You confirm in the trailer.\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canary_model.transcribe('../mono_output.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuartzNet15x5Base-En\n",
      "asr_talknet_aligner\n",
      "commandrecognition_en_matchboxnet3x1x64_v1\n",
      "commandrecognition_en_matchboxnet3x1x64_v2\n",
      "commandrecognition_en_matchboxnet3x1x64_v2_subset_task\n",
      "commandrecognition_en_matchboxnet3x2x64_v1\n",
      "commandrecognition_en_matchboxnet3x2x64_v2\n",
      "commandrecognition_en_matchboxnet3x2x64_v2_subset_task\n",
      "stt_be_conformer_ctc_large\n",
      "stt_be_conformer_transducer_large\n",
      "stt_by_fastconformer_hybrid_large_pc\n",
      "stt_ca_conformer_ctc_large\n",
      "stt_ca_conformer_transducer_large\n",
      "stt_ca_quartznet15x5\n",
      "stt_de_citrinet_1024\n",
      "stt_de_conformer_ctc_large\n",
      "stt_de_conformer_transducer_large\n",
      "stt_de_contextnet_1024\n",
      "stt_de_fastconformer_hybrid_large_pc\n",
      "stt_de_quartznet15x5\n",
      "stt_en_citrinet_1024\n",
      "stt_en_citrinet_1024_gamma_0_25\n",
      "stt_en_citrinet_256\n",
      "stt_en_citrinet_256_gamma_0_25\n",
      "stt_en_citrinet_512\n",
      "stt_en_citrinet_512_gamma_0_25\n",
      "stt_en_conformer_ctc_large\n",
      "stt_en_conformer_ctc_large_ls\n",
      "stt_en_conformer_ctc_medium\n",
      "stt_en_conformer_ctc_medium_ls\n",
      "stt_en_conformer_ctc_small\n",
      "stt_en_conformer_ctc_small_ls\n",
      "stt_en_conformer_ctc_xlarge\n",
      "stt_en_conformer_transducer_large\n",
      "stt_en_conformer_transducer_large_ls\n",
      "stt_en_conformer_transducer_medium\n",
      "stt_en_conformer_transducer_small\n",
      "stt_en_conformer_transducer_xlarge\n",
      "stt_en_conformer_transducer_xxlarge\n",
      "stt_en_contextnet_1024\n",
      "stt_en_contextnet_1024_mls\n",
      "stt_en_contextnet_256\n",
      "stt_en_contextnet_256_mls\n",
      "stt_en_contextnet_512\n",
      "stt_en_contextnet_512_mls\n",
      "stt_en_fastconformer_ctc_large\n",
      "stt_en_fastconformer_ctc_large_ls\n",
      "stt_en_fastconformer_ctc_xlarge\n",
      "stt_en_fastconformer_ctc_xxlarge\n",
      "stt_en_fastconformer_hybrid_large_pc\n",
      "stt_en_fastconformer_hybrid_large_streaming_1040ms\n",
      "stt_en_fastconformer_hybrid_large_streaming_480ms\n",
      "stt_en_fastconformer_hybrid_large_streaming_80ms\n",
      "stt_en_fastconformer_hybrid_large_streaming_multi\n",
      "stt_en_fastconformer_transducer_large\n",
      "stt_en_fastconformer_transducer_large_ls\n",
      "stt_en_fastconformer_transducer_xlarge\n",
      "stt_en_fastconformer_transducer_xxlarge\n",
      "stt_en_jasper10x5dr\n",
      "stt_en_quartznet15x5\n",
      "stt_en_squeezeformer_ctc_large_ls\n",
      "stt_en_squeezeformer_ctc_medium_large_ls\n",
      "stt_en_squeezeformer_ctc_medium_ls\n",
      "stt_en_squeezeformer_ctc_small_ls\n",
      "stt_en_squeezeformer_ctc_small_medium_ls\n",
      "stt_en_squeezeformer_ctc_xsmall_ls\n",
      "stt_enes_conformer_ctc_large\n",
      "stt_enes_conformer_ctc_large_codesw\n",
      "stt_enes_conformer_transducer_large\n",
      "stt_enes_conformer_transducer_large_codesw\n",
      "stt_enes_contextnet_large\n",
      "stt_eo_conformer_ctc_large\n",
      "stt_eo_conformer_transducer_large\n",
      "stt_es_citrinet_1024_gamma_0_25\n",
      "stt_es_citrinet_512\n",
      "stt_es_conformer_ctc_large\n",
      "stt_es_conformer_transducer_large\n",
      "stt_es_contextnet_1024\n",
      "stt_es_fastconformer_hybrid_large_pc\n",
      "stt_es_quartznet15x5\n",
      "stt_fr_citrinet_1024_gamma_0_25\n",
      "stt_fr_conformer_ctc_large\n",
      "stt_fr_conformer_transducer_large\n",
      "stt_fr_contextnet_1024\n",
      "stt_fr_fastconformer_hybrid_large_pc\n",
      "stt_fr_no_hyphen_citrinet_1024_gamma_0_25\n",
      "stt_fr_no_hyphen_conformer_ctc_large\n",
      "stt_fr_quartznet15x5\n",
      "stt_hi_conformer_ctc_medium\n",
      "stt_hr_conformer_ctc_large\n",
      "stt_hr_conformer_transducer_large\n",
      "stt_hr_fastconformer_hybrid_large_pc\n",
      "stt_it_conformer_ctc_large\n",
      "stt_it_conformer_transducer_large\n",
      "stt_it_fastconformer_hybrid_large_pc\n",
      "stt_it_quartznet15x5\n",
      "stt_kab_conformer_transducer_large\n",
      "stt_mr_conformer_ctc_medium\n",
      "stt_multilingual_fastconformer_hybrid_large_pc\n",
      "stt_multilingual_fastconformer_hybrid_large_pc_blend_eu\n",
      "stt_pl_fastconformer_hybrid_large_pc\n",
      "stt_pl_quartznet15x5\n",
      "stt_ru_conformer_ctc_large\n",
      "stt_ru_conformer_transducer_large\n",
      "stt_ru_fastconformer_hybrid_large_pc\n",
      "stt_ru_quartznet15x5\n",
      "stt_rw_conformer_ctc_large\n",
      "stt_rw_conformer_transducer_large\n",
      "stt_ua_fastconformer_hybrid_large_pc\n",
      "stt_zh_citrinet_1024_gamma_0_25\n",
      "stt_zh_citrinet_512\n",
      "stt_zh_conformer_transducer_large\n",
      "vad_marblenet\n",
      "vad_multilingual_frame_marblenet\n",
      "vad_multilingual_marblenet\n",
      "vad_telephony_marblenet\n"
     ]
    }
   ],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "for i in nemo_asr.models.ASRModel.list_available_models():\n",
    "\tprint(i.pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ClusteringDiarizer.list_available_models())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
