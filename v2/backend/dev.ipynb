{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe413600-2eb1-4075-9c86-1a5cbca78103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc1cdd6b-e612-4feb-bbd0-c0572381c787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raid/miniconda3/envs/nemo/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from speech_parser import Audio\n",
    "from speech_parser import SileroVAD\n",
    "from speech_parser import OnlineSpeakerClustering, LongformOnlineClustering, OverclusteringWindow, OverclusteringWindowArray\n",
    "from speech_parser import MSDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b890dd4-99b5-4959-93f1-005a62fd8628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "284b8f9b-2a1f-4739-9cb0-66c633d63cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ocw = OverClusteringWindow(\n",
    "#     torch.rand((200, 5, 192)),\n",
    "#     200,\n",
    "#     10\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07c91ae3-57cb-46b9-8636-e8d54b1d5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocwa = OverclusteringWindowArray(\n",
    "    200,\n",
    "    10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a05b437-6252-4cf6-aa7b-799e9dbe3cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocwa.extend(torch.rand((190, 5, 192)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fd6ff49-c67a-42ad-8956-014c629d0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocwa.extend(torch.rand((190, 5, 192)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05271b03-8117-4c4a-ab1f-aa1970b993ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocwa.cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de5e142f-4d76-446d-9c4e-63868f5be367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " tensor([[[0.4373, 0.5625, 0.4244,  ..., 0.5105, 0.4075, 0.5176],\n",
       "          [0.5439, 0.4519, 0.4497,  ..., 0.6418, 0.6001, 0.4598],\n",
       "          [0.3894, 0.4823, 0.6746,  ..., 0.5124, 0.6527, 0.4943],\n",
       "          [0.4959, 0.4621, 0.5806,  ..., 0.2801, 0.5456, 0.5923],\n",
       "          [0.3999, 0.5344, 0.5104,  ..., 0.3273, 0.5813, 0.5128]],\n",
       " \n",
       "         [[0.4311, 0.5054, 0.5438,  ..., 0.4864, 0.5182, 0.5056],\n",
       "          [0.4996, 0.5318, 0.5446,  ..., 0.5316, 0.5031, 0.4916],\n",
       "          [0.5266, 0.4742, 0.4993,  ..., 0.5150, 0.5412, 0.4782],\n",
       "          [0.4836, 0.4938, 0.4927,  ..., 0.5085, 0.5613, 0.4875],\n",
       "          [0.4735, 0.4863, 0.5251,  ..., 0.5326, 0.5031, 0.4610]],\n",
       " \n",
       "         [[0.2279, 0.3383, 0.4647,  ..., 0.5850, 0.2709, 0.1860],\n",
       "          [0.3699, 0.5175, 0.2676,  ..., 0.2744, 0.4843, 0.5058],\n",
       "          [0.2739, 0.3667, 0.3566,  ..., 0.4996, 0.4259, 0.6970],\n",
       "          [0.1517, 0.5809, 0.5900,  ..., 0.7510, 0.5007, 0.2700],\n",
       "          [0.2672, 0.2586, 0.5311,  ..., 0.4930, 0.4192, 0.7236]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.4890, 0.3291, 0.4179,  ..., 0.6559, 0.6141, 0.6672],\n",
       "          [0.3471, 0.3825, 0.5070,  ..., 0.5744, 0.5704, 0.4307],\n",
       "          [0.5918, 0.4890, 0.5085,  ..., 0.4974, 0.5203, 0.6840],\n",
       "          [0.4355, 0.3522, 0.6355,  ..., 0.5205, 0.6075, 0.6126],\n",
       "          [0.5833, 0.3625, 0.3718,  ..., 0.5431, 0.4132, 0.5577]],\n",
       " \n",
       "         [[0.5213, 0.3214, 0.7216,  ..., 0.3660, 0.4984, 0.5895],\n",
       "          [0.6289, 0.6462, 0.2689,  ..., 0.5305, 0.7758, 0.5540],\n",
       "          [0.4086, 0.5021, 0.6676,  ..., 0.5310, 0.3347, 0.4251],\n",
       "          [0.3509, 0.6643, 0.4244,  ..., 0.4382, 0.4361, 0.5884],\n",
       "          [0.4774, 0.6395, 0.5222,  ..., 0.5431, 0.3605, 0.6297]],\n",
       " \n",
       "         [[0.5136, 0.4060, 0.5370,  ..., 0.6283, 0.4672, 0.5860],\n",
       "          [0.4545, 0.4931, 0.4465,  ..., 0.4874, 0.4418, 0.4403],\n",
       "          [0.5212, 0.3429, 0.4646,  ..., 0.5855, 0.4926, 0.4768],\n",
       "          [0.4285, 0.4051, 0.5032,  ..., 0.5632, 0.5517, 0.4165],\n",
       "          [0.5235, 0.4801, 0.6563,  ..., 0.6311, 0.4709, 0.4949]]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocwa.grouped_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31a8367f-c1ee-4303-b2b3-2e15141c3fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 2, 1, 7, 1, 1, 2, 0, 1, 1, 8, 1, 1, 9, 1, 4, 1, 1, 1, 2, 1, 6, 1,\n",
       "        2, 1, 7, 2, 1, 1, 6, 2, 2, 6, 2, 7, 1, 5, 1, 2, 1, 1, 2, 1, 0, 6, 4, 1,\n",
       "        0, 2, 1, 1, 1, 2, 1, 2, 4, 2, 1, 0, 3, 1, 1, 9, 0, 1, 1, 6, 1, 2, 1, 1,\n",
       "        6, 8, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 3, 1, 1, 9, 2, 9, 1, 6,\n",
       "        1, 1, 1, 1, 2, 6, 1, 1, 6, 1, 2, 1, 1, 1, 0, 1, 2, 0, 1, 1, 1, 2, 6, 1,\n",
       "        4, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 7, 4, 8, 1, 1, 5, 2, 6, 1, 1, 1, 0, 1,\n",
       "        2, 2, 2, 2, 2, 5, 1, 2, 9, 1, 1, 5, 2, 1, 2, 1, 0, 5, 2, 1, 2, 2, 9, 2,\n",
       "        1, 1, 3, 1, 1, 2, 0, 1, 2, 0, 7, 1, 1, 2, 0, 9, 2, 2, 3, 1, 1, 8, 1, 1,\n",
       "        1, 9, 1, 1, 1, 1, 0, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocw.cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "433659b2-5c73-411f-9e47-d2bc0f82463a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 2, 1, 7, 1, 1, 2, 0, 1, 1, 8, 1, 1, 9, 1, 4, 1, 1, 1, 2, 1, 6, 1,\n",
       "        2, 1, 7, 2, 1, 1, 6, 2, 2, 6, 2, 7, 1, 5, 1, 2, 1, 1, 2, 1, 0, 6, 4, 1,\n",
       "        0, 2, 1, 1, 1, 2, 1, 2, 4, 2, 1, 0, 3, 1, 1, 9, 0, 1, 1, 6, 1, 2, 1, 1,\n",
       "        6, 8, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 3, 1, 1, 9, 2, 9, 1, 6,\n",
       "        1, 1, 1, 1, 2, 6, 1, 1, 6, 1, 2, 1, 1, 1, 0, 1, 2, 0, 1, 1, 1, 2, 6, 1,\n",
       "        4, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 7, 4, 8, 1, 1, 5, 2, 6, 1, 1, 1, 0, 1,\n",
       "        2, 2, 2, 2, 2, 5, 1, 2, 9, 1, 1, 5, 2, 1, 2, 1, 0, 5, 2, 1, 2, 2, 9, 2,\n",
       "        1, 1, 3, 1, 1, 2, 0, 1, 2, 0, 7, 1, 1, 2, 0, 9, 2, 2, 3, 1, 1, 8, 1, 1,\n",
       "        1, 9, 1, 1, 1, 1, 0, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocw.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4de376c-e4d9-4f15-835c-3f9278a042d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 3, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 3, 0, 0, 0,\n",
       "        3, 0, 0, 3, 0, 0, 0, 3, 3, 0, 3, 0, 0, 0, 0, 3, 0, 0, 3, 0, 4, 0, 2, 0,\n",
       "        4, 3, 0, 0, 0, 3, 0, 3, 2, 3, 0, 4, 0, 0, 0, 0, 4, 0, 0, 0, 0, 3, 0, 0,\n",
       "        0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0,\n",
       "        0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 4, 0, 3, 4, 0, 0, 0, 3, 0, 0,\n",
       "        2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, 0, 0, 0, 0, 4, 0,\n",
       "        3, 3, 3, 3, 3, 0, 0, 3, 0, 0, 0, 0, 3, 0, 3, 0, 4, 0, 3, 0, 3, 3, 0, 3,\n",
       "        0, 0, 0, 0, 0, 3, 4, 0, 3, 4, 0, 0, 0, 3, 4, 0, 3, 3, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 4, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocw.get_labels(\n",
    "    {\n",
    "        0: 4,\n",
    "        2: 3,\n",
    "        4: 2,\n",
    "        3: 0\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaeb9079-4f01-4e3c-89c3-fe9439cf8021",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ocw.get_grouped_embeddings(\n",
    "    {\n",
    "        0: 4,\n",
    "        2: 3,\n",
    "        4: 2,\n",
    "        3: 0\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5f7c2f1-368f-4bdc-bfec-4b4c89314d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "torch.Size([5, 192])\n",
      "tensor(2)\n",
      "torch.Size([5, 192])\n",
      "tensor(3)\n",
      "torch.Size([5, 192])\n",
      "tensor(4)\n",
      "torch.Size([5, 192])\n"
     ]
    }
   ],
   "source": [
    "for key, val in a.items():\n",
    "    print(key)\n",
    "    print(val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9b816c7-db8f-4b42-adb7-efa170cc7dde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-21 16:01:18 cloud:58] Found existing object /home/raid/.cache/torch/NeMo/NeMo_2.1.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo.\n",
      "[NeMo I 2025-02-21 16:01:18 cloud:64] Re-using file from: /home/raid/.cache/torch/NeMo/NeMo_2.1.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo\n",
      "[NeMo I 2025-02-21 16:01:18 common:826] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-02-21 16:01:19 modelPT:176] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: null\n",
      "    emb_dir: null\n",
      "    sample_rate: 16000\n",
      "    num_spks: 2\n",
      "    soft_label_thres: 0.5\n",
      "    labels: null\n",
      "    batch_size: 15\n",
      "    emb_batch_size: 0\n",
      "    shuffle: true\n",
      "    \n",
      "[NeMo W 2025-02-21 16:01:19 modelPT:183] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: null\n",
      "    emb_dir: null\n",
      "    sample_rate: 16000\n",
      "    num_spks: 2\n",
      "    soft_label_thres: 0.5\n",
      "    labels: null\n",
      "    batch_size: 15\n",
      "    emb_batch_size: 0\n",
      "    shuffle: false\n",
      "    \n",
      "[NeMo W 2025-02-21 16:01:19 modelPT:189] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    emb_dir: null\n",
      "    sample_rate: 16000\n",
      "    num_spks: 2\n",
      "    soft_label_thres: 0.5\n",
      "    labels: null\n",
      "    batch_size: 15\n",
      "    emb_batch_size: 0\n",
      "    shuffle: false\n",
      "    seq_eval_mode: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-21 16:01:19 features:305] PADDING: 16\n",
      "[NeMo I 2025-02-21 16:01:19 features:305] PADDING: 16\n",
      "[NeMo I 2025-02-21 16:01:20 save_restore_connector:275] Model EncDecDiarLabelModel was successfully restored from /home/raid/.cache/torch/NeMo/NeMo_2.1.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo.\n"
     ]
    }
   ],
   "source": [
    "msdd = MSDD(\n",
    "    threshold=0.8\n",
    ")\n",
    "titanet_l = msdd.speech_embedding_model\n",
    "vad = SileroVAD(\n",
    "    threshold=0.5\n",
    ")\n",
    "osc = OnlineSpeakerClustering()\n",
    "\n",
    "scales = [1.5, 1.25, 1.0, 0.75, 0.5]\n",
    "hops = [scale/4 for scale in scales]\n",
    "a = Audio(\n",
    "    scales, \n",
    "    hops, \n",
    "    speech_embedding_model=titanet_l,\n",
    "    voice_activity_detection_model=vad,\n",
    "    multi_scale_diarization_model=msdd,\n",
    "    speaker_clustering=osc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03f11f42-78eb-4c32-98a6-90bbc8145ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# waveform, sr = load_audio('test.wav')\n",
    "waveform, sr = load_audio('toefl_eg.mp3')\n",
    "waveform = waveform[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e921416-6557-4d4f-a3e5-31bb9a169bce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proba, labels = a(waveform[:100_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2037126f-d0e0-4918-ab7c-d7c7d4b7558e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ec85c7c-e61f-49ca-b320-8064d3590156",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba, labels = a(waveform[100_000:200_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7f833d8-3609-4d82-b272-0a06dc54277d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bbadaec6-9366-457e-80d7-d0457d8e933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba, labels = a(waveform[200_000:300_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bd5925f-18d4-45d5-ba7c-3951e4416fc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proba, labels = a(waveform[200_000:1_000_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d8a51f-1138-47a4-b6ff-8a2f095c69b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "247a64d2-e6ac-4daf-87d8-7dd0320d2aed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proba, labels = a(waveform[1_000_000:4_000_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8e029e6-652b-4198-8ec1-45a685747a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a(waveform[4_000_000:5_000_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "385c26d6-7d74-4f49-8756-d4b72b7f591a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged = a.get_merged_speaker_segments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c25b969-f6e8-4191-ad48-fecdf86b9540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SpeakerSegment(start=0, end=17.625, data=tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.0015, -0.0009, -0.0007]), mask=tensor([0., 0., 0.,  ..., 0., 0., 0.]), speaker=0, transcription=None)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2db251b8-7aad-40ca-a0f7-563387a549de",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.max_silence_per_segment_pct = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68c8241f-398e-4239-9aed-f06b6aa512c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.clear_merged_segments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6fa5cf8-b9a1-4d84-bd44-d5ef843cd6c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17.625,\n",
       " 32.5,\n",
       " 11.0,\n",
       " 1.0,\n",
       " 72.5,\n",
       " 4.75,\n",
       " 4.625,\n",
       " 11.5,\n",
       " 3.625,\n",
       " 10.5,\n",
       " 10.75,\n",
       " 10.25,\n",
       " 4.5,\n",
       " 1.0,\n",
       " 6.375,\n",
       " 19.75,\n",
       " 5.875,\n",
       " 10.25,\n",
       " 6.25]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s.duration for s in merged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc57ab2f-33d7-4cc6-94d1-3309cc7c3d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5.0, 5.5, tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       " (5.125, 5.625, tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       " (5.25, 5.75, tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       " (5.375, 5.875, tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       " (5.5, 6.0, tensor([1., 1., 1.,  ..., 0., 0., 0.])),\n",
       " (5.625, 6.125, tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       " (6.125, 6.625, tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       " (6.25, 6.75, tensor([0., 0., 0.,  ..., 1., 1., 1.])),\n",
       " (6.375, 6.875, tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       " (6.5, 7.0, tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       " (6.625, 7.125, tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       " (6.75, 7.25, tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       " (6.875, 7.375, tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       " (7.0, 7.5, tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       " (7.125, 7.625, tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       " (7.25, 7.75, tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       " (7.375, 7.875, tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       " (7.5, 8.0, tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       " (7.625, 8.125, tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       " (7.75, 8.25, tensor([1., 1., 1.,  ..., 1., 1., 1.]))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(s.start, s.end, s.mask) for s in a.base_scale_segments[40:60]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f05da305-b8ee-45b8-81df-ca5001b176eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000,  ..., 0.3207, 0.2932, 0.2494])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02ba2619-8f1e-4ef0-a7fa-3095a3d76154",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.base_scale_segments[44].has_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebb7c0b0-edb6-4732-97e7-c8378ad2f1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SpeakerSegment(start=0, end=17.625, data=tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.0015, -0.0009, -0.0007]), mask=tensor([0., 0., 0.,  ..., 0., 0., 0.]), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=17.875, end=50.375, data=tensor([-0.0037, -0.0012,  0.0005,  ...,  0.0037,  0.0032,  0.0023]), mask=tensor([0., 0., 0.,  ..., 0., 0., 0.]), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=50.875, end=61.875, data=tensor([ 1.6737e-03,  3.6514e-04, -5.9805e-05,  ..., -1.1843e-03,\n",
       "         -1.0885e-03, -7.7725e-04]), mask=tensor([0., 0., 0.,  ..., 0., 0., 0.]), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=77.0, end=78.0, data=tensor([-6.1943e-02, -6.3724e-02, -6.5221e-02,  ...,  3.0402e-04,\n",
       "          1.6657e-05,  2.6669e-04]), mask=tensor([1., 1., 1.,  ..., 0., 0., 0.]), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=62.375, end=134.875, data=tensor([-0.0970, -0.0802, -0.1046,  ...,  0.0000,  0.0000,  0.0000]), mask=tensor([1., 1., 1.,  ..., 0., 0., 0.]), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=136.25, end=141.0, data=tensor([ 1.0500e-04,  3.5520e-05, -9.4904e-07,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00]), mask=tensor([0., 0., 0.,  ..., 0., 0., 0.]), speaker=3, transcription=None),\n",
       " SpeakerSegment(start=141.375, end=146.0, data=tensor([0., 0., 0.,  ..., 0., 0., 0.]), mask=tensor([0., 0., 0.,  ..., 0., 0., 0.]), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=145.875, end=157.375, data=tensor([ 1.2453e-04,  1.5457e-04,  1.4144e-04,  ..., -2.8281e-05,\n",
       "          7.9136e-06, -5.3958e-06]), mask=tensor([0., 0., 0.,  ..., 0., 0., 0.]), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=157.25, end=160.875, data=tensor([-2.6874e-05,  4.8968e-05,  4.4306e-05,  ..., -5.7348e-02,\n",
       "         -2.9683e-02, -5.8608e-04]), mask=tensor([0., 0., 0.,  ..., 1., 1., 1.]), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=159.375, end=169.875, data=tensor([-0.0878, -0.0867, -0.0853,  ..., -0.1917, -0.0960,  0.0198]), mask=tensor([1., 1., 1.,  ..., 1., 1., 1.]), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=168.375, end=179.125, data=tensor([ 0.0931,  0.1584,  0.1427,  ..., -0.0321, -0.0633, -0.0969]), mask=tensor([1., 1., 1.,  ..., 1., 1., 1.]), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=178.375, end=188.625, data=tensor([ 0.0142,  0.0137,  0.0131,  ...,  0.0124, -0.0337, -0.0813]), mask=tensor([1., 1., 1.,  ..., 1., 1., 1.]), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=187.625, end=192.125, data=tensor([-6.4458e-02,  1.3279e-02,  4.3242e-02,  ...,  3.0467e-06,\n",
       "         -2.6407e-06, -3.0603e-05]), mask=tensor([1., 1., 1.,  ..., 0., 0., 0.]), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=197.375, end=198.375, data=tensor([-1.1186e-04, -7.7183e-05, -2.9783e-05,  ...,  5.9441e-02,\n",
       "          6.6492e-02,  7.0678e-02]), mask=tensor([0., 0., 0.,  ..., 1., 1., 1.]), speaker=3, transcription=None),\n",
       " SpeakerSegment(start=192.125, end=198.5, data=tensor([-3.4056e-05, -1.7974e-05,  1.6205e-06,  ...,  5.7088e-02,\n",
       "          5.9136e-02,  5.0103e-02]), mask=tensor([0., 0., 0.,  ..., 1., 1., 1.]), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=196.75, end=216.5, data=tensor([-0.0132, -0.0089, -0.0087,  ...,  0.0102, -0.0265, -0.0531]), mask=tensor([1., 1., 1.,  ..., 1., 1., 1.]), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=215.75, end=221.625, data=tensor([-0.0072, -0.0053, -0.0029,  ...,  0.0184,  0.0198,  0.0213]), mask=tensor([1., 1., 1.,  ..., 1., 1., 1.]), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=220.375, end=230.625, data=tensor([0.0920, 0.0868, 0.0808,  ..., 0.0118, 0.0401, 0.0730]), mask=tensor([1., 1., 1.,  ..., 1., 1., 1.]), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=229.625, end=235.875, data=tensor([ 0.0250,  0.0402,  0.0522,  ..., -0.1206, -0.1139, -0.1085]), mask=tensor([1., 1., 1.,  ..., 1., 1., 1.]), speaker=1, transcription=None)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b7ca19b-103a-455d-b670-2987d396b8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 6\n",
    "torchaudio.save('segmented.wav', merged[i].data.unsqueeze(0).cpu(), 16_000)\n",
    "merged[i].speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "976395de-af9c-4cfd-bf50-0490f8f93343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged[i].duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff5d6312-e33b-4ccb-ac12-7ab7d3b5aace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7168., device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.base_scale_segments[1].mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "307f2f0c-f5ac-4763-b46e-35842a013bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from speech_parser.audio import SpeakerSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "63c22ced-61c0-4140-8e09-443a96d64a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 16_000\n",
    "\n",
    "def merge_segments(segments, speaker, sampling_rate=16000):\n",
    "    \n",
    "    first_segment = segments[0]\n",
    "    start = first_segment.start\n",
    "    end = first_segment.start\n",
    "    normal_segment_duration = first_segment.end - first_segment.start\n",
    "    waveforms = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        increment = segment.end - end\n",
    "        assert increment <= normal_segment_duration, f\"Issue in the matching section of get_merged_speaker_segments(), {increment} > {normal_segment_duration}\"\n",
    "\n",
    "        concat_length = int(increment*sampling_rate)\n",
    "        waveforms.append(segment.data[-concat_length:])\n",
    "        \n",
    "        end = segment.end\n",
    "        \n",
    "    waveform = torch.cat(waveforms)\n",
    "    return SpeakerSegment(start, end, waveform, speaker)\n",
    "    # print([segment.start for segment in segments])\n",
    "    # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "04a760d7-b394-45e7-862b-6ddb25d60367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8000])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "295996a7-f2c4-4b53-be8e-180558e9710a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "active_speakers  = {}\n",
    "\n",
    "merged_segments = []\n",
    "\n",
    "for segment in a.base_scale_segments:\n",
    "    curr_active_speakers = list(active_speakers.items())\n",
    "    for speaker, active_segments in curr_active_speakers:\n",
    "        # end the speaker segment if the start of the new segment is past the end of the old\n",
    "        if (speaker in segment.speakers) and (segment.start <= active_segments[-1].end):\n",
    "            active_speakers[speaker].append(segment)\n",
    "        else:\n",
    "            # merge and end speakers\n",
    "            merged_segments.append(merge_segments(active_segments, speaker))\n",
    "            active_speakers.pop(speaker)\n",
    "        \n",
    "    for speaker in segment.speakers:\n",
    "        if speaker not in active_speakers:\n",
    "            # start speaker\n",
    "            active_speakers[speaker] = [segment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5d6ea473-af5a-4f14-bbbb-59ebad40d8dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SpeakerSegment(start=62.5, end=120.75, data=tensor([-0.0164, -0.0267,  0.0036,  ..., -0.0011, -0.0012, -0.0008],\n",
       "        device='cuda:0'), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=120.75, end=134.75, data=tensor([-0.0006, -0.0006, -0.0004,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        device='cuda:0'), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=136.25, end=138.5, data=tensor([ 1.0500e-04,  3.5520e-05, -9.4904e-07,  ..., -1.4732e-02,\n",
       "         -1.2964e-02, -1.2140e-02], device='cuda:0'), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=136.25, end=141.0, data=tensor([ 1.0500e-04,  3.5520e-05, -9.4904e-07,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=141.25, end=145.75, data=tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -7.6305e-05,\n",
       "         -5.3419e-05, -3.3926e-05], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=146.0, end=156.0, data=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=156.0, end=157.25, data=tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -2.2885e-05,\n",
       "         -1.2899e-04, -1.2129e-04], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=157.25, end=160.75, data=tensor([-2.6874e-05,  4.8968e-05,  4.4306e-05,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=159.5, end=169.5, data=tensor([0.0007, 0.0007, 0.0006,  ..., 0.0028, 0.0056, 0.0049], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=172.25, end=173.5, data=tensor([ 0.0203,  0.0203,  0.0234,  ..., -0.0372, -0.0215, -0.0152],\n",
       "        device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=175.75, end=176.5, data=tensor([-2.4743e-07, -8.0860e-08, -1.4174e-07,  ...,  1.9525e-03,\n",
       "         -1.7611e-03,  2.3732e-04], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=169.0, end=179.0, data=tensor([-1.4334e-07,  6.6419e-07, -6.3802e-07,  ..., -7.3732e-02,\n",
       "         -5.8260e-02, -1.3655e-02], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=178.5, end=182.5, data=tensor([-3.8050e-05, -6.3207e-06, -1.4694e-05,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=182.5, end=188.0, data=tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.0001, -0.0002, -0.0001],\n",
       "        device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=188.0, end=192.0, data=tensor([-7.9522e-05, -6.6163e-05, -8.6511e-05,  ...,  1.0326e-04,\n",
       "          5.8267e-05,  6.1283e-05], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=191.5, end=192.0, data=tensor([1.3016e-02, 1.4583e-02, 1.4826e-02,  ..., 1.0326e-04, 5.8267e-05,\n",
       "         6.1283e-05], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=192.0, end=192.5, data=tensor([ 2.2082e-05,  4.5894e-06, -6.2505e-07,  ..., -7.1008e-02,\n",
       "         -6.8520e-02, -6.5419e-02], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=192.0, end=197.75, data=tensor([ 2.2082e-05,  4.5894e-06, -6.2505e-07,  ...,  2.8127e-01,\n",
       "          1.7621e-01,  4.8635e-02], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=197.0, end=207.5, data=tensor([-0.0008, -0.0008, -0.0014,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=207.5, end=216.5, data=tensor([ 0.0000,  0.0000,  0.0000,  ...,  0.0102, -0.0265, -0.0531],\n",
       "        device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=215.75, end=221.0, data=tensor([-7.2389e-03, -5.2507e-03, -2.9456e-03,  ...,  6.4661e-07,\n",
       "          5.1318e-07,  7.4244e-08], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=221.0, end=221.5, data=tensor([-2.1241e-08,  2.0710e-07, -3.6155e-08,  ..., -1.8223e-01,\n",
       "         -2.0810e-01, -1.8929e-01], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=221.0, end=230.5, data=tensor([-2.1241e-08,  2.0710e-07, -3.6155e-08,  ..., -3.0790e-02,\n",
       "          5.9832e-02,  1.9181e-01], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=230.0, end=235.75, data=tensor([-5.8362e-05, -5.8233e-05, -7.1750e-05,  ...,  2.9137e-01,\n",
       "          2.7412e-01,  2.5115e-01], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=235.25, end=249.75, data=tensor([ 3.2089e-05,  1.9223e-05,  1.6009e-06,  ..., -1.4414e-15,\n",
       "         -1.5969e-15, -1.7658e-15], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=250.0, end=254.25, data=tensor([ 0.1177,  0.0553,  0.0043,  ..., -0.0002, -0.0002, -0.0003],\n",
       "        device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=253.75, end=258.75, data=tensor([-4.8572e-08, -2.0561e-05, -8.9008e-06,  ..., -8.3839e-05,\n",
       "         -1.2448e-04, -8.9330e-05], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=258.5, end=274.75, data=tensor([-8.2369e-05, -8.9729e-05, -9.1564e-05,  ..., -7.3750e-02,\n",
       "         -5.1632e-02, -2.4496e-02], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=274.0, end=284.0, data=tensor([-1.0222e-02, -5.7292e-03, -1.7429e-02,  ..., -1.3761e-05,\n",
       "         -2.5257e-06, -1.0457e-05], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=284.0, end=284.5, data=tensor([ 3.3601e-06,  2.0928e-05,  4.8903e-06,  ..., -1.2895e-03,\n",
       "          6.0876e-04, -2.2165e-03], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=284.0, end=292.5, data=tensor([ 3.3601e-06,  2.0928e-05,  4.8903e-06,  ..., -9.0860e-02,\n",
       "         -8.8710e-02, -8.3112e-02], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=292.25, end=295.25, data=tensor([-4.7058e-06, -7.2782e-06,  1.1720e-07,  ...,  2.0162e-05,\n",
       "          5.9449e-05,  6.3034e-06], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=294.75, end=305.25, data=tensor([ 0.0007,  0.0006,  0.0008,  ..., -0.0040, -0.0055, -0.0072],\n",
       "        device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=305.0, end=312.25, data=tensor([-2.8620e-06, -2.6328e-05, -3.5366e-05,  ...,  6.8589e-06,\n",
       "          2.8502e-06, -2.2640e-06], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=312.5, end=324.25, data=tensor([-9.5128e-05, -7.0577e-05, -7.0041e-05,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=325.5, end=328.75, data=tensor([0.0010, 0.0010, 0.0010,  ..., 0.0031, 0.0034, 0.0029], device='cuda:0'), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=325.75, end=328.75, data=tensor([-0.0030, -0.0030, -0.0028,  ...,  0.0031,  0.0034,  0.0029],\n",
       "        device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=354.25, end=355.5, data=tensor([0.0000, 0.0000, 0.0000,  ..., 0.0365, 0.0428, 0.0479], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=354.25, end=360.75, data=tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  4.7843e-18,\n",
       "          8.4065e-17, -1.4272e-17], device='cuda:0'), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=384.25, end=388.0, data=tensor([1.3542e-06, 1.3099e-06, 1.3898e-06,  ..., 1.9312e-03, 1.8659e-03,\n",
       "         1.6952e-03], device='cuda:0'), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=414.0, end=420.5, data=tensor([-0.0003, -0.0005, -0.0006,  ...,  0.0017,  0.0018,  0.0020],\n",
       "        device='cuda:0'), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=443.5, end=448.25, data=tensor([0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0004, 0.0004], device='cuda:0'), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=445.5, end=456.25, data=tensor([-3.6142e-02, -5.1562e-02, -6.2405e-02,  ..., -1.2034e-08,\n",
       "          1.1610e-08, -5.7967e-09], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=456.25, end=461.0, data=tensor([-2.9490e-09,  2.8441e-09, -2.2450e-09,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=461.0, end=463.0, data=tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -2.0518e-05,\n",
       "         -3.0218e-06, -4.8026e-07], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=463.25, end=464.0, data=tensor([ 3.7538e-06, -9.7592e-07,  9.4208e-07,  ...,  3.2053e-03,\n",
       "          3.1729e-03,  3.1985e-03], device='cuda:0'), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=464.0, end=466.0, data=tensor([3.8616e-03, 3.7273e-03, 2.8335e-03,  ..., 2.8540e-05, 4.3866e-05,\n",
       "         3.8670e-05], device='cuda:0'), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=463.25, end=473.0, data=tensor([ 3.7538e-06, -9.7592e-07,  9.4208e-07,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=503.25, end=507.0, data=tensor([-0.0001, -0.0001, -0.0001,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=507.5, end=508.0, data=tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.1063, -0.0710, -0.0395],\n",
       "        device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=507.5, end=518.25, data=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=518.25, end=524.25, data=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=524.5, end=529.5, data=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=529.5, end=542.5, data=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=542.75, end=579.75, data=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=579.75, end=582.25, data=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=582.25, end=590.0, data=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=590.0, end=592.5, data=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=592.5, end=607.5, data=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=607.5, end=624.75, data=tensor([0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5411e-05, 1.0687e-04,\n",
       "         2.0200e-04], device='cuda:0'), speaker=2, transcription=None)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e4a3217b-ff2d-4e4a-8f36-89800344abf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4973, device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_segments[-2].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e772f0b-869a-4e07-a948-30966824bc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "590ed1cb-114e-49ae-838b-2d9cb2868b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker 0: 62.00s - 120.00s (duration: 58.00s)\n",
      "Speaker 0: 120.25s - 134.00s (duration: 13.75s)\n",
      "Speaker 0: 135.75s - 137.75s (duration: 2.00s)\n",
      "Speaker 1: 135.75s - 140.25s (duration: 4.50s)\n",
      "Speaker 1: 140.75s - 145.00s (duration: 4.25s)\n",
      "Speaker 2: 145.50s - 155.25s (duration: 9.75s)\n",
      "Speaker 2: 155.50s - 156.50s (duration: 1.00s)\n",
      "Speaker 1: 156.75s - 160.00s (duration: 3.25s)\n",
      "Speaker 2: 159.00s - 168.75s (duration: 9.75s)\n",
      "Speaker 1: 168.50s - 178.25s (duration: 9.75s)\n",
      "Speaker 2: 171.75s - 172.75s (duration: 1.00s)\n",
      "Speaker 2: 175.25s - 175.75s (duration: 0.50s)\n",
      "Speaker 2: 178.00s - 181.75s (duration: 3.75s)\n",
      "Speaker 2: 182.00s - 187.25s (duration: 5.25s)\n",
      "Speaker 1: 187.50s - 191.25s (duration: 3.75s)\n",
      "Speaker 2: 191.00s - 191.25s (duration: 0.25s)\n",
      "Speaker 1: 191.50s - 191.75s (duration: 0.25s)\n",
      "Speaker 2: 191.50s - 197.00s (duration: 5.50s)\n",
      "Speaker 1: 196.50s - 206.75s (duration: 10.25s)\n",
      "Speaker 1: 207.00s - 215.75s (duration: 8.75s)\n",
      "Speaker 2: 215.25s - 220.25s (duration: 5.00s)\n",
      "Speaker 1: 220.50s - 229.75s (duration: 9.25s)\n",
      "Speaker 2: 220.50s - 220.75s (duration: 0.25s)\n",
      "Speaker 2: 229.50s - 235.00s (duration: 5.50s)\n",
      "Speaker 1: 234.75s - 249.00s (duration: 14.25s)\n",
      "Speaker 1: 249.25s - 253.25s (duration: 4.00s)\n",
      "Speaker 2: 253.00s - 257.75s (duration: 4.75s)\n",
      "Speaker 1: 257.75s - 273.75s (duration: 16.00s)\n",
      "Speaker 2: 273.25s - 283.00s (duration: 9.75s)\n",
      "Speaker 1: 283.25s - 291.50s (duration: 8.25s)\n",
      "Speaker 2: 283.25s - 283.50s (duration: 0.25s)\n",
      "Speaker 2: 291.50s - 294.25s (duration: 2.75s)\n",
      "Speaker 1: 294.00s - 304.25s (duration: 10.25s)\n",
      "Speaker 2: 304.25s - 311.25s (duration: 7.00s)\n",
      "Speaker 1: 311.75s - 323.25s (duration: 11.50s)\n",
      "Speaker 0: 324.75s - 327.75s (duration: 3.00s)\n",
      "Speaker 1: 325.00s - 327.75s (duration: 2.75s)\n",
      "Speaker 0: 353.50s - 359.75s (duration: 6.25s)\n",
      "Speaker 1: 353.50s - 354.50s (duration: 1.00s)\n",
      "Speaker 0: 383.50s - 387.00s (duration: 3.50s)\n",
      "Speaker 0: 413.25s - 419.50s (duration: 6.25s)\n",
      "Speaker 0: 442.75s - 447.25s (duration: 4.50s)\n",
      "Speaker 1: 444.75s - 455.25s (duration: 10.50s)\n",
      "Speaker 1: 455.50s - 460.00s (duration: 4.50s)\n",
      "Speaker 1: 460.25s - 462.00s (duration: 1.75s)\n",
      "Speaker 0: 462.50s - 463.00s (duration: 0.50s)\n",
      "Speaker 1: 462.50s - 472.00s (duration: 9.50s)\n",
      "Speaker 0: 463.25s - 465.00s (duration: 1.75s)\n",
      "Speaker 1: 502.50s - 506.00s (duration: 3.50s)\n",
      "Speaker 1: 506.75s - 507.00s (duration: 0.25s)\n",
      "Speaker 2: 506.75s - 517.25s (duration: 10.50s)\n",
      "Speaker 2: 517.50s - 523.25s (duration: 5.75s)\n",
      "Speaker 2: 523.75s - 528.50s (duration: 4.75s)\n",
      "Speaker 2: 528.75s - 541.50s (duration: 12.75s)\n",
      "Speaker 2: 542.00s - 578.75s (duration: 36.75s)\n",
      "Speaker 2: 579.00s - 581.25s (duration: 2.25s)\n",
      "Speaker 2: 581.50s - 589.00s (duration: 7.50s)\n",
      "Speaker 2: 589.25s - 591.50s (duration: 2.25s)\n",
      "Speaker 2: 591.75s - 606.50s (duration: 14.75s)\n",
      "Speaker 2: 606.75s - 623.75s (duration: 17.00s)\n"
     ]
    }
   ],
   "source": [
    "def process_timeline(data):\n",
    "    # Convert None to empty list for consistency\n",
    "    timeline = [[] if x is None else sorted(x) for x in data]\n",
    "    return timeline\n",
    "\n",
    "def merge_segments(timeline):\n",
    "    merged_segments = []\n",
    "    \n",
    "    # Find continuous segments for each speaker\n",
    "    for speaker in set([spk for t in timeline for spk in t]):\n",
    "        start_idx = None\n",
    "        \n",
    "        for t, speakers in enumerate(timeline):\n",
    "            if speaker in speakers:\n",
    "                if start_idx is None:\n",
    "                    start_idx = t\n",
    "            elif start_idx is not None:\n",
    "                # Add segment\n",
    "                merged_segments.append({\n",
    "                    'speaker': speaker,\n",
    "                    'start': start_idx * 0.25,  # Convert to seconds\n",
    "                    'end': t * 0.25,            # Convert to seconds\n",
    "                    'duration': (t - start_idx) * 0.25\n",
    "                })\n",
    "                start_idx = None\n",
    "        \n",
    "        # Handle segment that goes until the end\n",
    "        if start_idx is not None:\n",
    "            merged_segments.append({\n",
    "                'speaker': speaker,\n",
    "                'start': start_idx * 0.25,\n",
    "                'end': len(timeline) * 0.25,\n",
    "                'duration': (len(timeline) - start_idx) * 0.25\n",
    "            })\n",
    "    \n",
    "    return sorted(merged_segments, key=lambda x: (x['start'], x['speaker']))\n",
    "\n",
    "def generate_rttm(segments):\n",
    "    rttm_lines = []\n",
    "    \n",
    "    for seg in segments:\n",
    "        rttm_line = f\"SPEAKER unknown 1 {seg['start']:.3f} {seg['duration']:.3f} <NA> <NA> SPEAKER_{seg['speaker']} <NA> <NA>\"\n",
    "        rttm_lines.append(rttm_line)\n",
    "    \n",
    "    return \"\\n\".join(rttm_lines)\n",
    "\n",
    "def create_visualization(segments, total_duration):\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    \n",
    "    # Get unique speakers and assign them y-coordinates\n",
    "    unique_speakers = sorted(set(seg['speaker'] for seg in segments))\n",
    "    speaker_to_y = {speaker: i for i, speaker in enumerate(unique_speakers)}\n",
    "    \n",
    "    # Create line segments for each speaker\n",
    "    for speaker in unique_speakers:\n",
    "        speaker_segments = [seg for seg in segments if seg['speaker'] == speaker]\n",
    "        \n",
    "        for seg in speaker_segments:\n",
    "            plt.hlines(\n",
    "                y=speaker_to_y[seg['speaker']],\n",
    "                xmin=seg['start'],\n",
    "                xmax=seg['end'],\n",
    "                linewidth=4,\n",
    "                label=f\"Speaker {speaker}\"\n",
    "            )\n",
    "    \n",
    "    # Set axis limits to show the full timeline\n",
    "    plt.xlim(0, total_duration)\n",
    "    plt.ylim(-0.5, len(unique_speakers) - 0.5)\n",
    "    \n",
    "    # Set y-axis labels\n",
    "    plt.yticks(\n",
    "        range(len(unique_speakers)),\n",
    "        [f\"Speaker {speaker}\" for speaker in unique_speakers]\n",
    "    )\n",
    "    \n",
    "    # Remove duplicate labels in legend\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Speakers\")\n",
    "    plt.title(\"Speaker Diarization Timeline\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add some padding to the plot\n",
    "    plt.margins(x=0.02)\n",
    "    \n",
    "    # Adjust layout to prevent label cutoff\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig('speaker_diarization.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Use your existing code to process the data and generate segments\n",
    "timeline = process_timeline(timeline)\n",
    "merged_segments = merge_segments(timeline)\n",
    "\n",
    "# Calculate total duration in seconds\n",
    "total_duration = len(timeline) * 0.25\n",
    "\n",
    "# Generate RTTM\n",
    "rttm_content = generate_rttm(merged_segments)\n",
    "with open('output.rttm', 'w') as f:\n",
    "    f.write(rttm_content)\n",
    "\n",
    "# Create visualization with total duration\n",
    "create_visualization(merged_segments, total_duration)\n",
    "\n",
    "# Print segments for verification\n",
    "for seg in merged_segments:\n",
    "    print(f\"Speaker {seg['speaker']}: {seg['start']:.2f}s - {seg['end']:.2f}s (duration: {seg['duration']:.2f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6b30a9-a069-489c-8b8a-d7796b7fdc07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
