{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe413600-2eb1-4075-9c86-1a5cbca78103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc1cdd6b-e612-4feb-bbd0-c0572381c787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raid/miniconda3/envs/nemo/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from speech_parser import Audio\n",
    "from speech_parser import SileroVAD\n",
    "from speech_parser import OnlineSpeakerClustering\n",
    "from speech_parser import MSDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b890dd4-99b5-4959-93f1-005a62fd8628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b816c7-db8f-4b42-adb7-efa170cc7dde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-21 14:17:54 cloud:58] Found existing object /home/raid/.cache/torch/NeMo/NeMo_2.1.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo.\n",
      "[NeMo I 2025-02-21 14:17:54 cloud:64] Re-using file from: /home/raid/.cache/torch/NeMo/NeMo_2.1.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo\n",
      "[NeMo I 2025-02-21 14:17:54 common:826] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-02-21 14:17:55 modelPT:176] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: null\n",
      "    emb_dir: null\n",
      "    sample_rate: 16000\n",
      "    num_spks: 2\n",
      "    soft_label_thres: 0.5\n",
      "    labels: null\n",
      "    batch_size: 15\n",
      "    emb_batch_size: 0\n",
      "    shuffle: true\n",
      "    \n",
      "[NeMo W 2025-02-21 14:17:55 modelPT:183] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: null\n",
      "    emb_dir: null\n",
      "    sample_rate: 16000\n",
      "    num_spks: 2\n",
      "    soft_label_thres: 0.5\n",
      "    labels: null\n",
      "    batch_size: 15\n",
      "    emb_batch_size: 0\n",
      "    shuffle: false\n",
      "    \n",
      "[NeMo W 2025-02-21 14:17:55 modelPT:189] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    emb_dir: null\n",
      "    sample_rate: 16000\n",
      "    num_spks: 2\n",
      "    soft_label_thres: 0.5\n",
      "    labels: null\n",
      "    batch_size: 15\n",
      "    emb_batch_size: 0\n",
      "    shuffle: false\n",
      "    seq_eval_mode: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-21 14:17:55 features:305] PADDING: 16\n",
      "[NeMo I 2025-02-21 14:17:55 features:305] PADDING: 16\n",
      "[NeMo I 2025-02-21 14:17:56 save_restore_connector:275] Model EncDecDiarLabelModel was successfully restored from /home/raid/.cache/torch/NeMo/NeMo_2.1.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo.\n"
     ]
    }
   ],
   "source": [
    "msdd = MSDD(\n",
    "    threshold=0.8\n",
    ")\n",
    "titanet_l = msdd.speech_embedding_model\n",
    "vad = SileroVAD(\n",
    "    threshold=0.5\n",
    ")\n",
    "osc = OnlineSpeakerClustering()\n",
    "\n",
    "scales = [1.5, 1.25, 1.0, 0.75, 0.5]\n",
    "hops = [scale/4 for scale in scales]\n",
    "a = Audio(\n",
    "    scales, \n",
    "    hops, \n",
    "    speech_embedding_model=titanet_l,\n",
    "    voice_activity_detection_model=vad,\n",
    "    multi_scale_diarization_model=msdd,\n",
    "    speaker_clustering=osc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03f11f42-78eb-4c32-98a6-90bbc8145ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# waveform, sr = load_audio('test.wav')\n",
    "waveform, sr = load_audio('toefl_eg.mp3')\n",
    "waveform = waveform[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e921416-6557-4d4f-a3e5-31bb9a169bce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'next_other_segment_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m proba, labels \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m500_000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git_dir/atlas/v2/backend/speech_parser/audio/audio.py:73\u001b[0m, in \u001b[0;36mAudio.__call__\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, waveform: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Process new audio input and perform diarization\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     new_segments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend_waveform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     empty_returns \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), torch\u001b[38;5;241m.\u001b[39mtensor([], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(new_segments) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/git_dir/atlas/v2/backend/speech_parser/audio/audio.py:216\u001b[0m, in \u001b[0;36mAudio.append_waveform\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaveform\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples:\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopulate_segment_scales\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git_dir/atlas/v2/backend/speech_parser/audio/audio.py:242\u001b[0m, in \u001b[0;36mAudio.populate_segment_scales\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegment_scales[scale]\u001b[38;5;241m.\u001b[39mextend(segments)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Update segment relationships and generate embeddings\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_segments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed()\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_scale_segments[orig_num_base_segments:]\n",
      "File \u001b[0;32m~/git_dir/atlas/v2/backend/speech_parser/audio/audio.py:151\u001b[0m, in \u001b[0;36mAudio.assign_segments\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m     segment\u001b[38;5;241m.\u001b[39mother_scale_segments[scale] \u001b[38;5;241m=\u001b[39m other_scale_segment\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# Move to next segment at other scale\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnext_other_segment_idx\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(other_scale):\n\u001b[1;32m    152\u001b[0m         segment\u001b[38;5;241m.\u001b[39mother_scale_segments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'next_other_segment_idx' is not defined"
     ]
    }
   ],
   "source": [
    "proba, labels = a(waveform[:500_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2037126f-d0e0-4918-ab7c-d7c7d4b7558e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bd5925f-18d4-45d5-ba7c-3951e4416fc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proba, labels = a(waveform[500_000:1_000_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "247a64d2-e6ac-4daf-87d8-7dd0320d2aed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proba, labels = a(waveform[1_000_000:4_000_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8e029e6-652b-4198-8ec1-45a685747a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a(waveform[4_000_000:5_000_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "385c26d6-7d74-4f49-8756-d4b72b7f591a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new seg\n",
      "tensor(0.9096, device='cuda:0')\n",
      "17.625\n",
      "new seg\n",
      "tensor(0.8798, device='cuda:0')\n",
      "32.5\n",
      "new seg\n",
      "tensor(0.8923, device='cuda:0')\n",
      "11.5\n",
      "new seg\n",
      "tensor(0.6760, device='cuda:0')\n",
      "0.5\n",
      "new seg\n",
      "tensor(0.4064, device='cuda:0')\n",
      "0.625\n",
      "new seg\n",
      "tensor(0.6704, device='cuda:0')\n",
      "0.625\n",
      "new seg\n",
      "tensor(0.2560, device='cuda:0')\n",
      "0.625\n",
      "new seg\n",
      "tensor(1., device='cuda:0')\n",
      "0.625\n",
      "new seg\n",
      "tensor(0.9177, device='cuda:0')\n",
      "58.375\n",
      "new seg\n",
      "tensor(0.1373, device='cuda:0')\n",
      "0.75\n",
      "new seg\n",
      "tensor(0.0660, device='cuda:0')\n",
      "0.5\n",
      "new seg\n",
      "tensor(0.8224, device='cuda:0')\n",
      "14.125\n",
      "new seg\n",
      "tensor(0.1160, device='cuda:0')\n",
      "0.5\n",
      "new seg\n",
      "tensor(0.7964, device='cuda:0')\n",
      "2.25\n",
      "new seg\n",
      "tensor(0.7882, device='cuda:0')\n",
      "4.75\n",
      "new seg\n",
      "tensor(0.7888, device='cuda:0')\n",
      "4.625\n",
      "new seg\n",
      "tensor(0.2020, device='cuda:0')\n",
      "0.5\n",
      "new seg\n",
      "tensor(0.1340, device='cuda:0')\n",
      "0.5\n",
      "new seg\n",
      "tensor(0.2320, device='cuda:0')\n",
      "0.625\n",
      "new seg\n",
      "tensor(0.7618, device='cuda:0')\n",
      "10.25\n",
      "new seg\n",
      "tensor(0.3724, device='cuda:0')\n",
      "1.375\n",
      "new seg\n",
      "tensor(0.2280, device='cuda:0')\n",
      "0.5\n",
      "new seg\n",
      "tensor(0.6728, device='cuda:0')\n",
      "3.625\n",
      "new seg\n",
      "tensor(0.8321, device='cuda:0')\n",
      "10.625\n",
      "new seg\n",
      "tensor(1., device='cuda:0')\n",
      "1.125\n",
      "new seg\n",
      "tensor(0.5164, device='cuda:0')\n",
      "1.125\n",
      "new seg\n",
      "tensor(0.8639, device='cuda:0')\n",
      "9.875\n",
      "new seg\n",
      "tensor(0.5600, device='cuda:0')\n",
      "2.0\n",
      "new seg\n",
      "tensor(0.3840, device='cuda:0')\n",
      "1.25\n",
      "new seg\n",
      "tensor(0.4189, device='cuda:0')\n",
      "1.375\n",
      "new seg\n",
      "tensor(0.7509, device='cuda:0')\n",
      "5.625\n",
      "new seg\n",
      "tensor(0.4827, device='cuda:0')\n",
      "0.75\n",
      "new seg\n",
      "tensor(0.7990, device='cuda:0')\n",
      "4.125\n",
      "new seg\n",
      "tensor(0.1180, device='cuda:0')\n",
      "0.5\n",
      "new seg\n",
      "tensor(0.0960, device='cuda:0')\n",
      "0.5\n",
      "new seg\n",
      "tensor(0.8322, device='cuda:0')\n",
      "6.125\n",
      "new seg\n",
      "tensor(0.7834, device='cuda:0')\n",
      "10.375\n",
      "new seg\n",
      "tensor(0.8109, device='cuda:0')\n",
      "9.25\n",
      "new seg\n",
      "tensor(0.7310, device='cuda:0')\n",
      "5.25\n",
      "new seg\n",
      "tensor(0.3920, device='cuda:0')\n",
      "0.625\n",
      "new seg\n",
      "tensor(0.8109, device='cuda:0')\n",
      "9.625\n",
      "new seg\n",
      "tensor(0.7542, device='cuda:0')\n",
      "6.25\n",
      "new seg\n",
      "tensor(0.8395, device='cuda:0')\n",
      "8.5\n",
      "new seg\n",
      "tensor(0.8306, device='cuda:0')\n",
      "6.375\n"
     ]
    }
   ],
   "source": [
    "merged = a.get_merged_speaker_segments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6fa5cf8-b9a1-4d84-bd44-d5ef843cd6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17.625,\n",
       " 32.5,\n",
       " 11.5,\n",
       " 58.375,\n",
       " 14.125,\n",
       " 2.25,\n",
       " 4.75,\n",
       " 4.625,\n",
       " 10.25,\n",
       " 1.375,\n",
       " 3.625,\n",
       " 10.625,\n",
       " 1.125,\n",
       " 1.125,\n",
       " 9.875,\n",
       " 2.0,\n",
       " 1.25,\n",
       " 1.375,\n",
       " 5.625,\n",
       " 4.125,\n",
       " 6.125,\n",
       " 10.375,\n",
       " 9.25,\n",
       " 5.25,\n",
       " 9.625,\n",
       " 6.25,\n",
       " 8.5,\n",
       " 6.375]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s.duration for s in merged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b7ca19b-103a-455d-b670-2987d396b8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 9\n",
    "torchaudio.save('segmented.wav', merged[i].data.unsqueeze(0).cpu(), 16_000)\n",
    "merged[i].speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "976395de-af9c-4cfd-bf50-0490f8f93343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.375"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged[i].duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff5d6312-e33b-4ccb-ac12-7ab7d3b5aace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7168., device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.base_scale_segments[1].mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "307f2f0c-f5ac-4763-b46e-35842a013bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from speech_parser.audio import SpeakerSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "63c22ced-61c0-4140-8e09-443a96d64a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 16_000\n",
    "\n",
    "def merge_segments(segments, speaker, sampling_rate=16000):\n",
    "    \n",
    "    first_segment = segments[0]\n",
    "    start = first_segment.start\n",
    "    end = first_segment.start\n",
    "    normal_segment_duration = first_segment.end - first_segment.start\n",
    "    waveforms = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        increment = segment.end - end\n",
    "        assert increment <= normal_segment_duration, f\"Issue in the matching section of get_merged_speaker_segments(), {increment} > {normal_segment_duration}\"\n",
    "\n",
    "        concat_length = int(increment*sampling_rate)\n",
    "        waveforms.append(segment.data[-concat_length:])\n",
    "        \n",
    "        end = segment.end\n",
    "        \n",
    "    waveform = torch.cat(waveforms)\n",
    "    return SpeakerSegment(start, end, waveform, speaker)\n",
    "    # print([segment.start for segment in segments])\n",
    "    # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "04a760d7-b394-45e7-862b-6ddb25d60367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8000])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "295996a7-f2c4-4b53-be8e-180558e9710a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "active_speakers  = {}\n",
    "\n",
    "merged_segments = []\n",
    "\n",
    "for segment in a.base_scale_segments:\n",
    "    curr_active_speakers = list(active_speakers.items())\n",
    "    for speaker, active_segments in curr_active_speakers:\n",
    "        # end the speaker segment if the start of the new segment is past the end of the old\n",
    "        if (speaker in segment.speakers) and (segment.start <= active_segments[-1].end):\n",
    "            active_speakers[speaker].append(segment)\n",
    "        else:\n",
    "            # merge and end speakers\n",
    "            merged_segments.append(merge_segments(active_segments, speaker))\n",
    "            active_speakers.pop(speaker)\n",
    "        \n",
    "    for speaker in segment.speakers:\n",
    "        if speaker not in active_speakers:\n",
    "            # start speaker\n",
    "            active_speakers[speaker] = [segment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5d6ea473-af5a-4f14-bbbb-59ebad40d8dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SpeakerSegment(start=62.5, end=120.75, data=tensor([-0.0164, -0.0267,  0.0036,  ..., -0.0011, -0.0012, -0.0008],\n",
       "        device='cuda:0'), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=120.75, end=134.75, data=tensor([-0.0006, -0.0006, -0.0004,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        device='cuda:0'), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=136.25, end=138.5, data=tensor([ 1.0500e-04,  3.5520e-05, -9.4904e-07,  ..., -1.4732e-02,\n",
       "         -1.2964e-02, -1.2140e-02], device='cuda:0'), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=136.25, end=141.0, data=tensor([ 1.0500e-04,  3.5520e-05, -9.4904e-07,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=141.25, end=145.75, data=tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -7.6305e-05,\n",
       "         -5.3419e-05, -3.3926e-05], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=146.0, end=156.0, data=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=156.0, end=157.25, data=tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -2.2885e-05,\n",
       "         -1.2899e-04, -1.2129e-04], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=157.25, end=160.75, data=tensor([-2.6874e-05,  4.8968e-05,  4.4306e-05,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=159.5, end=169.5, data=tensor([0.0007, 0.0007, 0.0006,  ..., 0.0028, 0.0056, 0.0049], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=172.25, end=173.5, data=tensor([ 0.0203,  0.0203,  0.0234,  ..., -0.0372, -0.0215, -0.0152],\n",
       "        device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=175.75, end=176.5, data=tensor([-2.4743e-07, -8.0860e-08, -1.4174e-07,  ...,  1.9525e-03,\n",
       "         -1.7611e-03,  2.3732e-04], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=169.0, end=179.0, data=tensor([-1.4334e-07,  6.6419e-07, -6.3802e-07,  ..., -7.3732e-02,\n",
       "         -5.8260e-02, -1.3655e-02], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=178.5, end=182.5, data=tensor([-3.8050e-05, -6.3207e-06, -1.4694e-05,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=182.5, end=188.0, data=tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.0001, -0.0002, -0.0001],\n",
       "        device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=188.0, end=192.0, data=tensor([-7.9522e-05, -6.6163e-05, -8.6511e-05,  ...,  1.0326e-04,\n",
       "          5.8267e-05,  6.1283e-05], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=191.5, end=192.0, data=tensor([1.3016e-02, 1.4583e-02, 1.4826e-02,  ..., 1.0326e-04, 5.8267e-05,\n",
       "         6.1283e-05], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=192.0, end=192.5, data=tensor([ 2.2082e-05,  4.5894e-06, -6.2505e-07,  ..., -7.1008e-02,\n",
       "         -6.8520e-02, -6.5419e-02], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=192.0, end=197.75, data=tensor([ 2.2082e-05,  4.5894e-06, -6.2505e-07,  ...,  2.8127e-01,\n",
       "          1.7621e-01,  4.8635e-02], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=197.0, end=207.5, data=tensor([-0.0008, -0.0008, -0.0014,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=207.5, end=216.5, data=tensor([ 0.0000,  0.0000,  0.0000,  ...,  0.0102, -0.0265, -0.0531],\n",
       "        device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=215.75, end=221.0, data=tensor([-7.2389e-03, -5.2507e-03, -2.9456e-03,  ...,  6.4661e-07,\n",
       "          5.1318e-07,  7.4244e-08], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=221.0, end=221.5, data=tensor([-2.1241e-08,  2.0710e-07, -3.6155e-08,  ..., -1.8223e-01,\n",
       "         -2.0810e-01, -1.8929e-01], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=221.0, end=230.5, data=tensor([-2.1241e-08,  2.0710e-07, -3.6155e-08,  ..., -3.0790e-02,\n",
       "          5.9832e-02,  1.9181e-01], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=230.0, end=235.75, data=tensor([-5.8362e-05, -5.8233e-05, -7.1750e-05,  ...,  2.9137e-01,\n",
       "          2.7412e-01,  2.5115e-01], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=235.25, end=249.75, data=tensor([ 3.2089e-05,  1.9223e-05,  1.6009e-06,  ..., -1.4414e-15,\n",
       "         -1.5969e-15, -1.7658e-15], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=250.0, end=254.25, data=tensor([ 0.1177,  0.0553,  0.0043,  ..., -0.0002, -0.0002, -0.0003],\n",
       "        device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=253.75, end=258.75, data=tensor([-4.8572e-08, -2.0561e-05, -8.9008e-06,  ..., -8.3839e-05,\n",
       "         -1.2448e-04, -8.9330e-05], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=258.5, end=274.75, data=tensor([-8.2369e-05, -8.9729e-05, -9.1564e-05,  ..., -7.3750e-02,\n",
       "         -5.1632e-02, -2.4496e-02], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=274.0, end=284.0, data=tensor([-1.0222e-02, -5.7292e-03, -1.7429e-02,  ..., -1.3761e-05,\n",
       "         -2.5257e-06, -1.0457e-05], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=284.0, end=284.5, data=tensor([ 3.3601e-06,  2.0928e-05,  4.8903e-06,  ..., -1.2895e-03,\n",
       "          6.0876e-04, -2.2165e-03], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=284.0, end=292.5, data=tensor([ 3.3601e-06,  2.0928e-05,  4.8903e-06,  ..., -9.0860e-02,\n",
       "         -8.8710e-02, -8.3112e-02], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=292.25, end=295.25, data=tensor([-4.7058e-06, -7.2782e-06,  1.1720e-07,  ...,  2.0162e-05,\n",
       "          5.9449e-05,  6.3034e-06], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=294.75, end=305.25, data=tensor([ 0.0007,  0.0006,  0.0008,  ..., -0.0040, -0.0055, -0.0072],\n",
       "        device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=305.0, end=312.25, data=tensor([-2.8620e-06, -2.6328e-05, -3.5366e-05,  ...,  6.8589e-06,\n",
       "          2.8502e-06, -2.2640e-06], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=312.5, end=324.25, data=tensor([-9.5128e-05, -7.0577e-05, -7.0041e-05,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=325.5, end=328.75, data=tensor([0.0010, 0.0010, 0.0010,  ..., 0.0031, 0.0034, 0.0029], device='cuda:0'), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=325.75, end=328.75, data=tensor([-0.0030, -0.0030, -0.0028,  ...,  0.0031,  0.0034,  0.0029],\n",
       "        device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=354.25, end=355.5, data=tensor([0.0000, 0.0000, 0.0000,  ..., 0.0365, 0.0428, 0.0479], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=354.25, end=360.75, data=tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  4.7843e-18,\n",
       "          8.4065e-17, -1.4272e-17], device='cuda:0'), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=384.25, end=388.0, data=tensor([1.3542e-06, 1.3099e-06, 1.3898e-06,  ..., 1.9312e-03, 1.8659e-03,\n",
       "         1.6952e-03], device='cuda:0'), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=414.0, end=420.5, data=tensor([-0.0003, -0.0005, -0.0006,  ...,  0.0017,  0.0018,  0.0020],\n",
       "        device='cuda:0'), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=443.5, end=448.25, data=tensor([0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0004, 0.0004], device='cuda:0'), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=445.5, end=456.25, data=tensor([-3.6142e-02, -5.1562e-02, -6.2405e-02,  ..., -1.2034e-08,\n",
       "          1.1610e-08, -5.7967e-09], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=456.25, end=461.0, data=tensor([-2.9490e-09,  2.8441e-09, -2.2450e-09,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=461.0, end=463.0, data=tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -2.0518e-05,\n",
       "         -3.0218e-06, -4.8026e-07], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=463.25, end=464.0, data=tensor([ 3.7538e-06, -9.7592e-07,  9.4208e-07,  ...,  3.2053e-03,\n",
       "          3.1729e-03,  3.1985e-03], device='cuda:0'), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=464.0, end=466.0, data=tensor([3.8616e-03, 3.7273e-03, 2.8335e-03,  ..., 2.8540e-05, 4.3866e-05,\n",
       "         3.8670e-05], device='cuda:0'), speaker=0, transcription=None),\n",
       " SpeakerSegment(start=463.25, end=473.0, data=tensor([ 3.7538e-06, -9.7592e-07,  9.4208e-07,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00], device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=503.25, end=507.0, data=tensor([-0.0001, -0.0001, -0.0001,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=507.5, end=508.0, data=tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.1063, -0.0710, -0.0395],\n",
       "        device='cuda:0'), speaker=1, transcription=None),\n",
       " SpeakerSegment(start=507.5, end=518.25, data=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=518.25, end=524.25, data=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=524.5, end=529.5, data=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=529.5, end=542.5, data=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=542.75, end=579.75, data=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=579.75, end=582.25, data=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=582.25, end=590.0, data=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=590.0, end=592.5, data=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=592.5, end=607.5, data=tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), speaker=2, transcription=None),\n",
       " SpeakerSegment(start=607.5, end=624.75, data=tensor([0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5411e-05, 1.0687e-04,\n",
       "         2.0200e-04], device='cuda:0'), speaker=2, transcription=None)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e4a3217b-ff2d-4e4a-8f36-89800344abf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4973, device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_segments[-2].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e772f0b-869a-4e07-a948-30966824bc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "590ed1cb-114e-49ae-838b-2d9cb2868b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker 0: 62.00s - 120.00s (duration: 58.00s)\n",
      "Speaker 0: 120.25s - 134.00s (duration: 13.75s)\n",
      "Speaker 0: 135.75s - 137.75s (duration: 2.00s)\n",
      "Speaker 1: 135.75s - 140.25s (duration: 4.50s)\n",
      "Speaker 1: 140.75s - 145.00s (duration: 4.25s)\n",
      "Speaker 2: 145.50s - 155.25s (duration: 9.75s)\n",
      "Speaker 2: 155.50s - 156.50s (duration: 1.00s)\n",
      "Speaker 1: 156.75s - 160.00s (duration: 3.25s)\n",
      "Speaker 2: 159.00s - 168.75s (duration: 9.75s)\n",
      "Speaker 1: 168.50s - 178.25s (duration: 9.75s)\n",
      "Speaker 2: 171.75s - 172.75s (duration: 1.00s)\n",
      "Speaker 2: 175.25s - 175.75s (duration: 0.50s)\n",
      "Speaker 2: 178.00s - 181.75s (duration: 3.75s)\n",
      "Speaker 2: 182.00s - 187.25s (duration: 5.25s)\n",
      "Speaker 1: 187.50s - 191.25s (duration: 3.75s)\n",
      "Speaker 2: 191.00s - 191.25s (duration: 0.25s)\n",
      "Speaker 1: 191.50s - 191.75s (duration: 0.25s)\n",
      "Speaker 2: 191.50s - 197.00s (duration: 5.50s)\n",
      "Speaker 1: 196.50s - 206.75s (duration: 10.25s)\n",
      "Speaker 1: 207.00s - 215.75s (duration: 8.75s)\n",
      "Speaker 2: 215.25s - 220.25s (duration: 5.00s)\n",
      "Speaker 1: 220.50s - 229.75s (duration: 9.25s)\n",
      "Speaker 2: 220.50s - 220.75s (duration: 0.25s)\n",
      "Speaker 2: 229.50s - 235.00s (duration: 5.50s)\n",
      "Speaker 1: 234.75s - 249.00s (duration: 14.25s)\n",
      "Speaker 1: 249.25s - 253.25s (duration: 4.00s)\n",
      "Speaker 2: 253.00s - 257.75s (duration: 4.75s)\n",
      "Speaker 1: 257.75s - 273.75s (duration: 16.00s)\n",
      "Speaker 2: 273.25s - 283.00s (duration: 9.75s)\n",
      "Speaker 1: 283.25s - 291.50s (duration: 8.25s)\n",
      "Speaker 2: 283.25s - 283.50s (duration: 0.25s)\n",
      "Speaker 2: 291.50s - 294.25s (duration: 2.75s)\n",
      "Speaker 1: 294.00s - 304.25s (duration: 10.25s)\n",
      "Speaker 2: 304.25s - 311.25s (duration: 7.00s)\n",
      "Speaker 1: 311.75s - 323.25s (duration: 11.50s)\n",
      "Speaker 0: 324.75s - 327.75s (duration: 3.00s)\n",
      "Speaker 1: 325.00s - 327.75s (duration: 2.75s)\n",
      "Speaker 0: 353.50s - 359.75s (duration: 6.25s)\n",
      "Speaker 1: 353.50s - 354.50s (duration: 1.00s)\n",
      "Speaker 0: 383.50s - 387.00s (duration: 3.50s)\n",
      "Speaker 0: 413.25s - 419.50s (duration: 6.25s)\n",
      "Speaker 0: 442.75s - 447.25s (duration: 4.50s)\n",
      "Speaker 1: 444.75s - 455.25s (duration: 10.50s)\n",
      "Speaker 1: 455.50s - 460.00s (duration: 4.50s)\n",
      "Speaker 1: 460.25s - 462.00s (duration: 1.75s)\n",
      "Speaker 0: 462.50s - 463.00s (duration: 0.50s)\n",
      "Speaker 1: 462.50s - 472.00s (duration: 9.50s)\n",
      "Speaker 0: 463.25s - 465.00s (duration: 1.75s)\n",
      "Speaker 1: 502.50s - 506.00s (duration: 3.50s)\n",
      "Speaker 1: 506.75s - 507.00s (duration: 0.25s)\n",
      "Speaker 2: 506.75s - 517.25s (duration: 10.50s)\n",
      "Speaker 2: 517.50s - 523.25s (duration: 5.75s)\n",
      "Speaker 2: 523.75s - 528.50s (duration: 4.75s)\n",
      "Speaker 2: 528.75s - 541.50s (duration: 12.75s)\n",
      "Speaker 2: 542.00s - 578.75s (duration: 36.75s)\n",
      "Speaker 2: 579.00s - 581.25s (duration: 2.25s)\n",
      "Speaker 2: 581.50s - 589.00s (duration: 7.50s)\n",
      "Speaker 2: 589.25s - 591.50s (duration: 2.25s)\n",
      "Speaker 2: 591.75s - 606.50s (duration: 14.75s)\n",
      "Speaker 2: 606.75s - 623.75s (duration: 17.00s)\n"
     ]
    }
   ],
   "source": [
    "def process_timeline(data):\n",
    "    # Convert None to empty list for consistency\n",
    "    timeline = [[] if x is None else sorted(x) for x in data]\n",
    "    return timeline\n",
    "\n",
    "def merge_segments(timeline):\n",
    "    merged_segments = []\n",
    "    \n",
    "    # Find continuous segments for each speaker\n",
    "    for speaker in set([spk for t in timeline for spk in t]):\n",
    "        start_idx = None\n",
    "        \n",
    "        for t, speakers in enumerate(timeline):\n",
    "            if speaker in speakers:\n",
    "                if start_idx is None:\n",
    "                    start_idx = t\n",
    "            elif start_idx is not None:\n",
    "                # Add segment\n",
    "                merged_segments.append({\n",
    "                    'speaker': speaker,\n",
    "                    'start': start_idx * 0.25,  # Convert to seconds\n",
    "                    'end': t * 0.25,            # Convert to seconds\n",
    "                    'duration': (t - start_idx) * 0.25\n",
    "                })\n",
    "                start_idx = None\n",
    "        \n",
    "        # Handle segment that goes until the end\n",
    "        if start_idx is not None:\n",
    "            merged_segments.append({\n",
    "                'speaker': speaker,\n",
    "                'start': start_idx * 0.25,\n",
    "                'end': len(timeline) * 0.25,\n",
    "                'duration': (len(timeline) - start_idx) * 0.25\n",
    "            })\n",
    "    \n",
    "    return sorted(merged_segments, key=lambda x: (x['start'], x['speaker']))\n",
    "\n",
    "def generate_rttm(segments):\n",
    "    rttm_lines = []\n",
    "    \n",
    "    for seg in segments:\n",
    "        rttm_line = f\"SPEAKER unknown 1 {seg['start']:.3f} {seg['duration']:.3f} <NA> <NA> SPEAKER_{seg['speaker']} <NA> <NA>\"\n",
    "        rttm_lines.append(rttm_line)\n",
    "    \n",
    "    return \"\\n\".join(rttm_lines)\n",
    "\n",
    "def create_visualization(segments, total_duration):\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    \n",
    "    # Get unique speakers and assign them y-coordinates\n",
    "    unique_speakers = sorted(set(seg['speaker'] for seg in segments))\n",
    "    speaker_to_y = {speaker: i for i, speaker in enumerate(unique_speakers)}\n",
    "    \n",
    "    # Create line segments for each speaker\n",
    "    for speaker in unique_speakers:\n",
    "        speaker_segments = [seg for seg in segments if seg['speaker'] == speaker]\n",
    "        \n",
    "        for seg in speaker_segments:\n",
    "            plt.hlines(\n",
    "                y=speaker_to_y[seg['speaker']],\n",
    "                xmin=seg['start'],\n",
    "                xmax=seg['end'],\n",
    "                linewidth=4,\n",
    "                label=f\"Speaker {speaker}\"\n",
    "            )\n",
    "    \n",
    "    # Set axis limits to show the full timeline\n",
    "    plt.xlim(0, total_duration)\n",
    "    plt.ylim(-0.5, len(unique_speakers) - 0.5)\n",
    "    \n",
    "    # Set y-axis labels\n",
    "    plt.yticks(\n",
    "        range(len(unique_speakers)),\n",
    "        [f\"Speaker {speaker}\" for speaker in unique_speakers]\n",
    "    )\n",
    "    \n",
    "    # Remove duplicate labels in legend\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Speakers\")\n",
    "    plt.title(\"Speaker Diarization Timeline\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add some padding to the plot\n",
    "    plt.margins(x=0.02)\n",
    "    \n",
    "    # Adjust layout to prevent label cutoff\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig('speaker_diarization.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Use your existing code to process the data and generate segments\n",
    "timeline = process_timeline(timeline)\n",
    "merged_segments = merge_segments(timeline)\n",
    "\n",
    "# Calculate total duration in seconds\n",
    "total_duration = len(timeline) * 0.25\n",
    "\n",
    "# Generate RTTM\n",
    "rttm_content = generate_rttm(merged_segments)\n",
    "with open('output.rttm', 'w') as f:\n",
    "    f.write(rttm_content)\n",
    "\n",
    "# Create visualization with total duration\n",
    "create_visualization(merged_segments, total_duration)\n",
    "\n",
    "# Print segments for verification\n",
    "for seg in merged_segments:\n",
    "    print(f\"Speaker {seg['speaker']}: {seg['start']:.2f}s - {seg['end']:.2f}s (duration: {seg['duration']:.2f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6b30a9-a069-489c-8b8a-d7796b7fdc07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
