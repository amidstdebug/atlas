{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7695f41-d60f-4a07-91da-0535a8874ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.environ.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3443b712-f8fe-41f5-bab2-a95ad1ece681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raid/miniconda3/envs/diart/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "EVAL_DATASET = \"voxconverse\"\n",
    "\n",
    "if EVAL_DATASET == \"callhome\":\n",
    "    ds = load_dataset(\"talkbank/callhome\", \"eng\", token=hf_token)\n",
    "    \n",
    "    train_testvalid = ds['data'].train_test_split(test_size=0.2, seed=0)\n",
    "    test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=0)\n",
    "    \n",
    "    ds = DatasetDict({\n",
    "        'train': train_testvalid['train'],\n",
    "        'validation': test_valid['test'],\n",
    "        'test': test_valid['train']\n",
    "    })\n",
    "\n",
    "elif EVAL_DATASET == \"ami\":\n",
    "    ds = load_dataset(\"diarizers-community/ami\", \"ihm\")\n",
    "elif EVAL_DATASET == \"voxconverse\":\n",
    "    ds = load_dataset(\"diarizers-community/voxconverse\")\n",
    "\n",
    "ds = ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c560d1-e31b-4367-bcef-fb47ff10248a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "/home/raid/miniconda3/envs/diart/lib/python3.12/site-packages/diart/audio.py:8: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from pyannote.metrics.diarization import DiarizationErrorRate\n",
    "from pyannote.core import Segment, Annotation\n",
    "\n",
    "from pipeline import OnlinePipeline, OnlinePipelineConfig\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0af5715-694e-4a24-ac30-dae29ed47a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "/home/raid/miniconda3/envs/diart/lib/python3.12/site-packages/speechbrain/utils/autocast.py:68: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "INFO:pipeline.utils:Checking if model Systran/faster-whisper-large-v3 is in cache\n",
      "INFO:pipeline.utils:Model Systran/faster-whisper-large-v3 found in cache at .cache/models--Systran--faster-whisper-large-v3/snapshots/edaa852ec7e145841d8ffdb056a99866b5f0a478\n"
     ]
    }
   ],
   "source": [
    "config = OnlinePipelineConfig()\n",
    "diar_pipeline = OnlinePipeline(config)\n",
    "diar_pipeline.batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e660e1d4-d310-4465-8b8a-e6d38d766771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reference_annotation(speakers, starts, ends):\n",
    "    reference = Annotation()\n",
    "    for speaker, start, end in zip(speakers, starts, ends):\n",
    "        segment = Segment(start, end)\n",
    "        reference[segment] = speaker\n",
    "    return reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04504ad8-7461-4ff5-bfad-e2ba48da7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_optimal_der(reference, hypothesis, collar=0.25, skip_overlap=False):\n",
    "    \"\"\"\n",
    "    Calculate DER with optimal mapping between reference and hypothesis speakers.\n",
    "    \"\"\"\n",
    "    # Initialize DER metric\n",
    "    der_metric = DiarizationErrorRate(collar=collar, skip_overlap=skip_overlap)\n",
    "    \n",
    "    # Get unique speakers in reference and hypothesis\n",
    "    ref_speakers = list(reference.labels())\n",
    "    hyp_speakers = list(hypothesis.labels())\n",
    "    \n",
    "    # If pyannote's optimal_mapping method is available, use it\n",
    "    if hasattr(der_metric, 'optimal_mapping'):\n",
    "        mapping = der_metric.optimal_mapping(reference, hypothesis)\n",
    "        remapped_hypothesis = hypothesis.rename_labels(mapping=mapping)\n",
    "        der = der_metric(reference, remapped_hypothesis)\n",
    "        metrics = der_metric.compute_components(reference, remapped_hypothesis)\n",
    "        return der, mapping, metrics\n",
    "        \n",
    "    # Otherwise, implement custom optimal mapping search\n",
    "    else:\n",
    "        best_der = float('inf')\n",
    "        best_mapping = None\n",
    "        best_metrics = None\n",
    "        \n",
    "        # If we have more hypothesis speakers than reference speakers\n",
    "        # We'll only map the most active hypothesis speakers\n",
    "        if len(hyp_speakers) > len(ref_speakers):\n",
    "            # Get duration of each speaker in hypothesis\n",
    "            speaker_durations = {}\n",
    "            for segment, _, label in hypothesis.itertracks(yield_label=True):\n",
    "                duration = segment.duration\n",
    "                speaker_durations[label] = speaker_durations.get(label, 0) + duration\n",
    "                \n",
    "            # Keep only the most active speakers\n",
    "            hyp_speakers = sorted(hyp_speakers, \n",
    "                                 key=lambda spk: speaker_durations.get(spk, 0), \n",
    "                                 reverse=True)[:len(ref_speakers)]\n",
    "            \n",
    "        # Try all possible mappings between hypothesis and reference speakers\n",
    "        for perm in permutations(ref_speakers, len(hyp_speakers)):\n",
    "            mapping = dict(zip(hyp_speakers, perm))\n",
    "            \n",
    "            # Create a remapped hypothesis\n",
    "            remapped = hypothesis.copy()\n",
    "            for segment, track, label in hypothesis.itertracks(yield_label=True):\n",
    "                if label in mapping:\n",
    "                    remapped[segment, track] = mapping[label]\n",
    "                    \n",
    "            # Calculate DER with this mapping\n",
    "            current_der = der_metric(reference, remapped)\n",
    "            current_metrics = der_metric.compute_components(reference, remapped)\n",
    "            \n",
    "            # Update if this is the best mapping so far\n",
    "            if current_der < best_der:\n",
    "                best_der = current_der\n",
    "                best_mapping = mapping\n",
    "                best_metrics = current_metrics\n",
    "                \n",
    "        return best_der, best_mapping, best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8db7f0e3-e875-4b80-b124-ca3b8859f47f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raid/miniconda3/envs/diart/lib/python3.12/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████| 232/232 [3:23:45<00:00, 52.70s/it, sample_no=232, der_before=0.136, der_after=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-sample metrics DataFrame:\n",
      "   sample_id  der_before  confusion_before  false_alarm_before  \\\n",
      "0          0    0.217296        204.667059           20.930000   \n",
      "1          1    0.266398        227.343333           39.806667   \n",
      "2          2    0.131683         47.040000            5.693134   \n",
      "3          3    0.142952         20.993333           27.556667   \n",
      "4          4    0.230829        185.413333            6.639801   \n",
      "\n",
      "   missed_detection_before  correct_before  total_before  der_after  \\\n",
      "0                13.002941      880.370000       1098.04   0.214585   \n",
      "1                19.550000      829.316667       1076.21   0.431914   \n",
      "2                 1.170000      361.130000        409.34   0.065886   \n",
      "3                16.990700      420.495967        458.48   0.134060   \n",
      "4                82.333333      920.953333       1188.70   0.225103   \n",
      "\n",
      "   confusion_after  false_alarm_after  missed_detection_after  correct_after  \\\n",
      "0       201.623726          20.613333               13.386274     883.030000   \n",
      "1       406.096667          39.683333               19.050000     651.063333   \n",
      "2        20.690000           5.609801                0.670000     387.980000   \n",
      "3        17.466667          27.900000               16.097366     424.915967   \n",
      "4       180.553333           6.639801               80.386667     927.760000   \n",
      "\n",
      "   total_after  \n",
      "0      1098.04  \n",
      "1      1076.21  \n",
      "2       409.34  \n",
      "3       458.48  \n",
      "4      1188.70  \n",
      "\n",
      "Summarized statistics DataFrame:\n",
      "                 metric         before          after\n",
      "0            Global DER       0.273818       0.216882\n",
      "1         Confusion (s)   29392.335808   21675.604266\n",
      "2       False Alarm (s)    4145.274710    4163.959039\n",
      "3  Missed Detection (s)    4094.155159    3967.278942\n",
      "4           Correct (s)  103946.909033  111790.516792\n",
      "5   Total Reference (s)  137433.400000  137433.400000\n",
      "6            Median DER       0.251420       0.207095\n",
      "7              Mean DER       0.268596       0.228902\n",
      "8           Std Dev DER       0.156129       0.142804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize DER metric\n",
    "der_metric = DiarizationErrorRate(collar=0.25, skip_overlap=False)\n",
    "\n",
    "sample_count = 0 \n",
    "pbar = tqdm(ds)\n",
    "\n",
    "sample_metrics = []\n",
    "\n",
    "for sample in pbar:\n",
    "    audio, starts, ends, speakers = sample['audio'], sample['timestamps_start'], sample['timestamps_end'], sample['speakers']\n",
    "    waveform, sample_rate = audio['array'], audio['sampling_rate']\n",
    "    waveform = np.expand_dims(waveform, 1)\n",
    "\n",
    "    diar_pipeline(waveform, sample_rate)\n",
    "    \n",
    "    hypothesis_before = diar_pipeline.get_annotation()\n",
    "    reference = create_reference_annotation(speakers, starts, ends)\n",
    "    \n",
    "    der_before, mapping_before, metrics_before = calculate_optimal_der(reference, hypothesis_before)\n",
    "    \n",
    "    diar_pipeline.reannotate()\n",
    "    hypothesis_after = diar_pipeline.get_annotation()\n",
    "    der_after, mapping_after, metrics_after = calculate_optimal_der(reference, hypothesis_after)\n",
    "    \n",
    "    sample_metrics.append({\n",
    "        'sample_id': sample_count,\n",
    "        'der_before': der_before,\n",
    "        'confusion_before': metrics_before['confusion'],\n",
    "        'false_alarm_before': metrics_before['false alarm'],\n",
    "        'missed_detection_before': metrics_before['missed detection'],\n",
    "        'correct_before': metrics_before['correct'],\n",
    "        'total_before': metrics_before['total'],\n",
    "        'der_after': der_after,\n",
    "        'confusion_after': metrics_after['confusion'],\n",
    "        'false_alarm_after': metrics_after['false alarm'],\n",
    "        'missed_detection_after': metrics_after['missed detection'],\n",
    "        'correct_after': metrics_after['correct'],\n",
    "        'total_after': metrics_after['total']\n",
    "    })\n",
    "    \n",
    "    sample_count += 1\n",
    "    pbar.set_postfix({'sample_no': sample_count, 'der_before': der_before, 'der_after': der_after})\n",
    "    \n",
    "    diar_pipeline.reset()\n",
    "\n",
    "df_samples = pd.DataFrame(sample_metrics)\n",
    "\n",
    "global_metrics = {\n",
    "    'metric': [\n",
    "        'Global DER',\n",
    "        'Confusion (s)',\n",
    "        'False Alarm (s)',\n",
    "        'Missed Detection (s)',\n",
    "        'Correct (s)',\n",
    "        'Total Reference (s)',\n",
    "        'Median DER',\n",
    "        'Mean DER',\n",
    "        'Std Dev DER'\n",
    "    ],\n",
    "    'before': [\n",
    "        (df_samples['confusion_before'].sum() + df_samples['false_alarm_before'].sum() + df_samples['missed_detection_before'].sum()) / df_samples['total_before'].sum(),\n",
    "        df_samples['confusion_before'].sum(),\n",
    "        df_samples['false_alarm_before'].sum(),\n",
    "        df_samples['missed_detection_before'].sum(),\n",
    "        df_samples['correct_before'].sum(),\n",
    "        df_samples['total_before'].sum(),\n",
    "        df_samples['der_before'].median(),\n",
    "        df_samples['der_before'].mean(),\n",
    "        df_samples['der_before'].std()\n",
    "    ],\n",
    "    'after': [\n",
    "        (df_samples['confusion_after'].sum() + df_samples['false_alarm_after'].sum() + df_samples['missed_detection_after'].sum()) / df_samples['total_after'].sum(),\n",
    "        df_samples['confusion_after'].sum(),\n",
    "        df_samples['false_alarm_after'].sum(),\n",
    "        df_samples['missed_detection_after'].sum(),\n",
    "        df_samples['correct_after'].sum(),\n",
    "        df_samples['total_after'].sum(),\n",
    "        df_samples['der_after'].median(),\n",
    "        df_samples['der_after'].mean(),\n",
    "        df_samples['der_after'].std()\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(global_metrics)\n",
    "\n",
    "print(\"Per-sample metrics DataFrame:\")\n",
    "print(df_samples.head())\n",
    "print(\"\\nSummarized statistics DataFrame:\")\n",
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afbb55db-3270-49b2-ac62-2b0eff6dc3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples.to_csv('sample_metrics.csv', index=False)\n",
    "df_summary.to_csv('summary_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5bbcca-79ba-476f-90b2-4553c87ee504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
